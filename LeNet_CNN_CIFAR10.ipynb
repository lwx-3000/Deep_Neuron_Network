{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import data and apply normalization"
      ],
      "metadata": {
        "id": "HPsmE8f5Rqhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QunzqHy_s5Ku",
        "outputId": "08098a42-c1a2-454d-b99e-c33bab1cd235"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Without Batch-Normalization"
      ],
      "metadata": {
        "id": "DlbHKsmuw1cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(10, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*6*6, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction using to convolutional layers.\n",
        "        x = self.avgpool(self.sigmoid(self.conv1(x)))\n",
        "        x = self.avgpool(self.sigmoid(self.conv2(x)))\n",
        "        #reshape the tensor to 1-d to fit the FC layer input\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        # Classifier using three fully connected layers.\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is available\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "\n",
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = LeNet().cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = LeNet()\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer\n",
        "lr= 0.41 # not fine-tuned lr.\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "\n",
        "    for (imgs, labels) in train_loader_iter:\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6E1dXT-rlvF",
        "outputId": "2af80a39-1464-4373-e880-3bc48dcf61a8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using CPU.\n",
            "epoch 1, loss 0.0181, train acc 0.102, test acc 0.100, time 24.1 sec\n",
            "epoch 2, loss 0.0180, train acc 0.101, test acc 0.102, time 24.1 sec\n",
            "epoch 3, loss 0.0172, train acc 0.168, test acc 0.229, time 24.2 sec\n",
            "epoch 4, loss 0.0159, train acc 0.241, test acc 0.269, time 24.1 sec\n",
            "epoch 5, loss 0.0155, train acc 0.264, test acc 0.286, time 24.4 sec\n",
            "epoch 6, loss 0.0152, train acc 0.276, test acc 0.252, time 24.4 sec\n",
            "epoch 7, loss 0.0148, train acc 0.306, test acc 0.350, time 24.1 sec\n",
            "epoch 8, loss 0.0140, train acc 0.352, test acc 0.306, time 24.2 sec\n",
            "epoch 9, loss 0.0135, train acc 0.374, test acc 0.345, time 24.0 sec\n",
            "epoch 10, loss 0.0130, train acc 0.394, test acc 0.403, time 24.0 sec\n",
            "epoch 11, loss 0.0127, train acc 0.409, test acc 0.427, time 24.2 sec\n",
            "epoch 12, loss 0.0124, train acc 0.424, test acc 0.444, time 24.0 sec\n",
            "epoch 13, loss 0.0120, train acc 0.439, test acc 0.462, time 23.9 sec\n",
            "epoch 14, loss 0.0117, train acc 0.458, test acc 0.475, time 24.0 sec\n",
            "epoch 15, loss 0.0114, train acc 0.470, test acc 0.472, time 23.9 sec\n",
            "epoch 16, loss 0.0111, train acc 0.483, test acc 0.478, time 24.2 sec\n",
            "epoch 17, loss 0.0109, train acc 0.495, test acc 0.487, time 24.4 sec\n",
            "epoch 18, loss 0.0107, train acc 0.505, test acc 0.510, time 24.1 sec\n",
            "epoch 19, loss 0.0105, train acc 0.518, test acc 0.484, time 24.4 sec\n",
            "epoch 20, loss 0.0103, train acc 0.525, test acc 0.491, time 25.2 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. with an additional droppout layer"
      ],
      "metadata": {
        "id": "5brh_OLmw7XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNetDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(10, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*6*6, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(self.sigmoid(self.conv1(x)))\n",
        "        x = self.avgpool(self.sigmoid(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nw_SnAQGxB-M"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "\n",
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = LeNet().cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = LeNet()\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer\n",
        "lr= 0.4 # not fine-tuned lr.\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "\n",
        "    for (imgs, labels) in train_loader_iter:\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CYm8hnsyRmA",
        "outputId": "bb1a7670-5954-4a6a-96f6-b42b36245e7d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using CPU.\n",
            "epoch 1, loss 0.0181, train acc 0.099, test acc 0.100, time 24.4 sec\n",
            "epoch 2, loss 0.0180, train acc 0.104, test acc 0.100, time 24.6 sec\n",
            "epoch 3, loss 0.0171, train acc 0.174, test acc 0.209, time 25.8 sec\n",
            "epoch 4, loss 0.0159, train acc 0.242, test acc 0.223, time 25.1 sec\n",
            "epoch 5, loss 0.0155, train acc 0.264, test acc 0.282, time 24.7 sec\n",
            "epoch 6, loss 0.0151, train acc 0.282, test acc 0.300, time 24.1 sec\n",
            "epoch 7, loss 0.0147, train acc 0.313, test acc 0.337, time 24.2 sec\n",
            "epoch 8, loss 0.0140, train acc 0.350, test acc 0.354, time 24.1 sec\n",
            "epoch 9, loss 0.0135, train acc 0.375, test acc 0.390, time 24.1 sec\n",
            "epoch 10, loss 0.0131, train acc 0.394, test acc 0.423, time 24.0 sec\n",
            "epoch 11, loss 0.0128, train acc 0.405, test acc 0.385, time 24.3 sec\n",
            "epoch 12, loss 0.0125, train acc 0.422, test acc 0.435, time 24.1 sec\n",
            "epoch 13, loss 0.0122, train acc 0.436, test acc 0.444, time 24.1 sec\n",
            "epoch 14, loss 0.0119, train acc 0.448, test acc 0.447, time 24.1 sec\n",
            "epoch 15, loss 0.0116, train acc 0.461, test acc 0.469, time 24.1 sec\n",
            "epoch 16, loss 0.0113, train acc 0.476, test acc 0.478, time 24.5 sec\n",
            "epoch 17, loss 0.0111, train acc 0.486, test acc 0.475, time 24.1 sec\n",
            "epoch 18, loss 0.0109, train acc 0.499, test acc 0.503, time 24.1 sec\n",
            "epoch 19, loss 0.0107, train acc 0.508, test acc 0.515, time 24.1 sec\n",
            "epoch 20, loss 0.0105, train acc 0.518, test acc 0.508, time 24.2 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pek7stjzpWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With one additional batch normalization"
      ],
      "metadata": {
        "id": "-ycHYbyRzbxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNetBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetBatchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(10)  # Batch normalization for 6 channels\n",
        "        self.conv2 = nn.Conv2d(10, 16, kernel_size=5)\n",
        "        self.bn2 = nn.BatchNorm2d(16)  # Batch normalization for 16 channels\n",
        "        self.fc1 = nn.Linear(16*6*6, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(self.sigmoid(self.bn1(self.conv1(x))))\n",
        "        x = self.avgpool(self.sigmoid(self.bn2(self.conv2(x))))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "EkDHMKcsziNz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    acc_sum,n = 0,0\n",
        "    for (imgs, labels) in data_iter:\n",
        "        # send data to the GPU if cuda is availabel\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "            n += labels.shape[0]\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "\n",
        "# Loading training set and test set using DataLoader.\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Training using GPU.')\n",
        "    net = LeNet().cuda()\n",
        "else:\n",
        "    print('Training using CPU.')\n",
        "    net = LeNet()\n",
        "\n",
        "#Initialize network parameters.\n",
        "net.apply(weights_init)\n",
        "\n",
        "#Loss function\n",
        "if torch.cuda.is_available():\n",
        "    loss = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train using SGD optimizer\n",
        "lr= 0.4 # not fine-tuned lr.\n",
        "opt_n = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# Training stage\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "\n",
        "    for (imgs, labels) in train_loader_iter:\n",
        "        net.train()\n",
        "        opt_n.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        # Label prediction from LeNet\n",
        "        y_hat = net(imgs)\n",
        "        l = loss(y_hat, labels)\n",
        "        # Backprobagation\n",
        "        l.backward()\n",
        "        opt_n.step()\n",
        "\n",
        "        # Calculate tarining error\n",
        "        with torch.no_grad():\n",
        "            labels = labels.long()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (torch.sum(torch.argmax(y_hat, dim=1) == labels)).float().item()\n",
        "            n += labels.shape[0]\n",
        "    # calculate testing error every epoch.\n",
        "    test_acc = evaluate_accuracy(iter(test_loader), net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "          % (epoch, train_l_sum/n, train_acc_sum/n, test_acc,\n",
        "            time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fL9yki6zqvo",
        "outputId": "bf13e249-dd79-4158-e239-59a2f7af25e9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using CPU.\n",
            "epoch 1, loss 0.0181, train acc 0.098, test acc 0.100, time 24.2 sec\n",
            "epoch 2, loss 0.0180, train acc 0.115, test acc 0.128, time 24.1 sec\n",
            "epoch 3, loss 0.0165, train acc 0.203, test acc 0.220, time 24.0 sec\n",
            "epoch 4, loss 0.0158, train acc 0.248, test acc 0.237, time 24.1 sec\n",
            "epoch 5, loss 0.0154, train acc 0.267, test acc 0.260, time 24.1 sec\n",
            "epoch 6, loss 0.0151, train acc 0.281, test acc 0.297, time 24.5 sec\n",
            "epoch 7, loss 0.0149, train acc 0.297, test acc 0.315, time 26.3 sec\n",
            "epoch 8, loss 0.0146, train acc 0.320, test acc 0.325, time 24.6 sec\n",
            "epoch 9, loss 0.0141, train acc 0.345, test acc 0.323, time 24.1 sec\n",
            "epoch 10, loss 0.0135, train acc 0.373, test acc 0.400, time 24.0 sec\n",
            "epoch 11, loss 0.0130, train acc 0.396, test acc 0.421, time 24.3 sec\n",
            "epoch 12, loss 0.0126, train acc 0.414, test acc 0.422, time 24.0 sec\n",
            "epoch 13, loss 0.0122, train acc 0.433, test acc 0.428, time 24.0 sec\n",
            "epoch 14, loss 0.0119, train acc 0.448, test acc 0.452, time 23.9 sec\n",
            "epoch 15, loss 0.0116, train acc 0.461, test acc 0.477, time 24.1 sec\n",
            "epoch 16, loss 0.0113, train acc 0.476, test acc 0.486, time 24.4 sec\n",
            "epoch 17, loss 0.0111, train acc 0.488, test acc 0.505, time 24.2 sec\n",
            "epoch 18, loss 0.0108, train acc 0.500, test acc 0.498, time 24.2 sec\n",
            "epoch 19, loss 0.0106, train acc 0.511, test acc 0.496, time 24.2 sec\n",
            "epoch 20, loss 0.0104, train acc 0.518, test acc 0.510, time 24.3 sec\n"
          ]
        }
      ]
    }
  ]
}