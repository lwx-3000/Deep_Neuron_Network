{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Law4IPg_MiAm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5j1a6blMm7H",
        "outputId": "226fa695-704a-4429-eea1-605e243fea7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 349455012.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 32259838.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 251779940.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7532830.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.311582\n",
            "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.251805\n",
            "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 2.244698\n",
            "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 2.171565\n",
            "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 2.118522\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.026911\n",
            "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 1.888034\n",
            "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 1.771437\n",
            "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 1.563168\n",
            "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 1.483950\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.336843\n",
            "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 1.219443\n",
            "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.953022\n",
            "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.963759\n",
            "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.870953\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.842245\n",
            "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.734265\n",
            "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.648495\n",
            "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.752979\n",
            "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.569876\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.610499\n",
            "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.560583\n",
            "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.580044\n",
            "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.630649\n",
            "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.512648\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.479327\n",
            "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.552186\n",
            "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.541850\n",
            "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.289136\n",
            "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.549855\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.478582\n",
            "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.413056\n",
            "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.436912\n",
            "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.433121\n",
            "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.364846\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.327204\n",
            "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.394337\n",
            "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.377535\n",
            "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.400428\n",
            "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.308591\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.338685\n",
            "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.365653\n",
            "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.329145\n",
            "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.296331\n",
            "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.469558\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.275853\n",
            "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.353121\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.352675\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.262724\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.330347\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.272024\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.384890\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.325638\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.348133\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.363767\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.353865\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.302085\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.415100\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.396405\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.408722\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.240068\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.355249\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.314051\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.342867\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.361158\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.312567\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.299033\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.396026\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.459262\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.274807\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.339961\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.363280\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.328306\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.270989\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.186394\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.415046\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.292584\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.352326\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.257916\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.243578\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.366252\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.367701\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.329406\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.365035\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.304539\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.326530\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.280867\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.286000\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.197372\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.288038\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.220805\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.249750\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.296460\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.261142\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9255/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.341703\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.339060\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.294601\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.260953\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.229504\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.274560\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.215125\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.427006\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.278812\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.295792\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.201352\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.234973\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.244256\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.158698\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.167225\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.237405\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.277613\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.226394\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.259509\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.281245\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.219316\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.228779\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.378102\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.254784\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.279192\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.415369\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.209232\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.227600\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.370875\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.180964\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.235241\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.179411\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.182856\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.307996\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.220314\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.183044\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.432284\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.225313\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.209853\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.190687\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.146576\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.302422\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.291261\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.191833\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.350855\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.108481\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.183889\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9365/10000 (94%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.247904\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.246439\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.242581\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.234624\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.240590\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.166629\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.196123\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.257678\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.184739\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.156767\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.296975\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.158932\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.190760\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.191390\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.157740\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.202584\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.170864\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.223168\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.313186\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.222823\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.240216\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.259102\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.344537\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.283522\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.198314\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.185664\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.154574\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.209331\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.209484\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.143533\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.116379\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.189505\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.199710\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.356406\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.237665\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.226801\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.179651\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.196752\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.168462\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.132150\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.152306\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.154944\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.238083\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.177118\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.190169\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.237819\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.221921\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9436/10000 (94%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.200495\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.195458\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.250001\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.163481\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.220277\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.182711\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.289079\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.289463\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.165374\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.177821\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.178755\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.302496\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.155513\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.126545\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.084610\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.196099\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.165104\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.133671\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.141804\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.164338\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.216622\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.310466\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.204519\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.216703\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.147033\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.202909\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.154908\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.162975\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.176717\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.281238\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.186047\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.186692\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.201611\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.283505\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.181925\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.243104\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.156870\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.155116\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.156853\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.116909\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.169986\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.237272\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.205178\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.157819\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.206856\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.219547\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.146916\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9494/10000 (95%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.167211\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.172501\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.151009\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.190445\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.119865\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.130756\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.249759\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.132581\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.109389\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.158414\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.130897\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.262533\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.161178\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.176693\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.231295\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.100757\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.213133\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.171247\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.185262\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.141550\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.160725\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.205069\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.114366\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.111424\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.136504\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.163316\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.162362\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.156494\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.149219\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.168442\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.198865\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.142340\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.099793\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.100430\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.138224\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.160613\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.155239\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.075123\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.154811\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.091652\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.159950\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.186735\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.074544\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.164882\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.106971\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.192009\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.079395\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9533/10000 (95%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.225123\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.124222\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.108269\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.144228\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.126004\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.187894\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.105350\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.098800\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.250936\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.125623\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.259987\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.066683\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.159335\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.087520\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.127950\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.133313\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.162224\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.135120\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.159585\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.088593\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.108455\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.112838\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.133306\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.130822\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.144690\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.120219\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.070429\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.186027\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.155124\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.203715\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.340232\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.280126\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.182118\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.103389\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.128128\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.117176\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.096876\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.078484\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.144823\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.102812\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.123030\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.113087\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.140593\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.112327\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.083632\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.084896\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.239146\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9600/10000 (96%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.098578\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.172097\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.106015\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.110394\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.115134\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.281641\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.140620\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.162648\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.152825\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.092346\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.178911\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.103743\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.136127\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.069674\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.171626\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.141218\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.166505\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.152069\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.143361\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.156267\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.127712\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.102503\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.098818\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.127579\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.174051\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.164242\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.106428\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.141597\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.102523\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.062275\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.091967\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.116581\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.134264\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.068556\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.137060\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.216938\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.084188\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.108162\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.172209\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.107292\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.095809\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.231312\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.074696\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.075075\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.115580\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.062091\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.126592\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9609/10000 (96%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.113561\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.150842\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.057337\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.094338\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.148788\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.120806\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.094236\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.107424\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.094459\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.106171\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.058612\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.071774\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.087250\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.158588\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.170295\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.111559\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.090433\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.107974\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.066696\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.038834\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.108287\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.092950\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.096614\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.077657\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.064118\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.074241\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.116713\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.068671\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.057760\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.122949\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.079905\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.206624\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.117277\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.093074\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.206136\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.128605\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.285826\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.050933\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.056004\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.171384\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.083093\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.095833\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.063249\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.160353\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.214086\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.072532\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.073042\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9647/10000 (96%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.090668\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.176679\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.091032\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.132356\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.084893\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.067396\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.208335\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.155358\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.104924\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.134660\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.101630\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.084449\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.106181\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.099926\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.113085\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.112163\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.092610\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.131240\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.100005\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.144795\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.132689\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.066410\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.064965\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.146317\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.107735\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.065341\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.270584\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.063528\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.071930\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.117218\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.035970\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.058438\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.118298\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.138997\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.203599\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.036500\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.111697\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.097135\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.131120\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.106212\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.055887\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.052310\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.086879\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.096360\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.104783\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.125372\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.137812\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9661/10000 (97%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Load MNIST dataset, normalize data. 0.1307 = mean, 0.3081 = sd (from kaggle)\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "#Create the NN model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 200)\n",
        "        self.fc2 = nn.Linear(200, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)  # log_softmax\n",
        "\n",
        "model = Net()\n",
        "\n",
        "#Loss Function and Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Training Loop\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "#Test\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "for epoch in range(10):  #10 epochs\n",
        "    train(epoch)\n",
        "    test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5cwvXMlPMYw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU0znE32ObE5"
      },
      "source": [
        "## Only use numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SjH7IjHJPPoC"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms # for download MNIST dataset and balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vu8taJgbPOYb"
      },
      "outputs": [],
      "source": [
        "#Download MNIST data and normalize it\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform = transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qFIHUwJHQ7yz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot\n",
        "%matplotlib inline\n",
        "from torchvision import datasets # Only use for download MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "0f2BYhGKQ9Ut",
        "outputId": "cfb56a4c-f808-4031-e5e1-257d951aaddd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAahElEQVR4nO3dcWyU9R3H8c8V6AHaXqmlvXYUbFFEBboNpaso4miALiMg/CHqEjAGBitm2Dm1iwK6Zd0wMiJhsCwKcxNwJAKBP0iw2hK3ggEhhGw2tOkEQ1uUjV4ptjD62x+EmydFfI67fnvH+5U8SXv3/HpfHx775ukdV59zzgkAgF6WYj0AAODGRIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJ/tYDfFV3d7dOnjyptLQ0+Xw+63EAAB4559Te3q68vDylpFz9OqfPBejkyZPKz8+3HgMAcJ1OnDihYcOGXfX+PhegtLQ0SZcGT09PN54GAOBVKBRSfn5++Pv51cQtQGvXrtUrr7yilpYWFRUVac2aNZowYcI1113+sVt6ejoBAoAEdq2nUeLyIoS3335bFRUVWr58uT766CMVFRVp2rRpOnXqVDweDgCQgOISoFWrVmnBggV64okndNddd2n9+vUaPHiw3njjjXg8HAAgAcU8QOfPn9fBgwdVWlr6/wdJSVFpaanq6uqu2L+rq0uhUChiAwAkv5gH6PPPP9fFixeVk5MTcXtOTo5aWlqu2L+qqkqBQCC88Qo4ALgxmP9D1MrKSrW1tYW3EydOWI8EAOgFMX8VXFZWlvr166fW1taI21tbWxUMBq/Y3+/3y+/3x3oMAEAfF/MroNTUVI0fP17V1dXh27q7u1VdXa2SkpJYPxwAIEHF5d8BVVRUaN68ebrnnns0YcIErV69Wh0dHXriiSfi8XAAgAQUlwA98sgj+uyzz7Rs2TK1tLTo29/+tnbv3n3FCxMAADcun3POWQ/xZaFQSIFAQG1tbbwTAgAkoG/6fdz8VXAAgBsTAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwEfMArVixQj6fL2IbPXp0rB8GAJDg+sfji95999169913//8g/ePyMACABBaXMvTv31/BYDAeXxoAkCTi8hzQsWPHlJeXp8LCQj3++OM6fvz4Vfft6upSKBSK2AAAyS/mASouLtbGjRu1e/durVu3Tk1NTXrggQfU3t7e4/5VVVUKBALhLT8/P9YjAQD6IJ9zzsXzAc6cOaMRI0Zo1apVevLJJ6+4v6urS11dXeHPQ6GQ8vPz1dbWpvT09HiOBgCIg1AopEAgcM3v43F/dUBGRoZGjRqlhoaGHu/3+/3y+/3xHgMA0MfE/d8BnT17Vo2NjcrNzY33QwEAEkjMA/TMM8+otrZW//rXv/T3v/9dDz/8sPr166dHH3001g8FAEhgMf8R3KeffqpHH31Up0+f1tChQ3X//fdr3759Gjp0aKwfCgCQwGIeoC1btsT6SwJ9WjSv4+ns7PS85j//+Y/nNZs3b/a8JlorVqzwvObs2bOe12RkZHhe8+c//9nzGkn64Q9/GNU6fDO8FxwAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLuv5AOsBDNm31KUl1dnec1O3bs8LxmzZo1ntf0dUOGDPG85tZbb/W8Jisry/Oa++67z/MaxB9XQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBu2EjKf3xj3+Mat3SpUtjO4ixzMzMqNZ95zvf8bxm/fr1ntcUFhZ6XoPkwRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCNyNFn/f88897XrNmzZo4TNIzv9/vec1f/vIXz2vuuusuz2sCgYDnNZKUm5sb1TrAC66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATvBkp+rwPPvjA85rOzs44TNKzrKwsz2tmz54dh0mAxMIVEADABAECAJjwHKC9e/dqxowZysvLk8/n0/bt2yPud85p2bJlys3N1aBBg1RaWqpjx47Fal4AQJLwHKCOjg4VFRVp7dq1Pd6/cuVKvfbaa1q/fr3279+vm266SdOmTevVn8kDAPo+zy9CKCsrU1lZWY/3Oee0evVqvfDCC5o5c6Yk6c0331ROTo62b9+uuXPnXt+0AICkEdPngJqamtTS0qLS0tLwbYFAQMXFxaqrq+txTVdXl0KhUMQGAEh+MQ1QS0uLJCknJyfi9pycnPB9X1VVVaVAIBDe8vPzYzkSAKCPMn8VXGVlpdra2sLbiRMnrEcCAPSCmAYoGAxKklpbWyNub21tDd/3VX6/X+np6REbACD5xTRABQUFCgaDqq6uDt8WCoW0f/9+lZSUxPKhAAAJzvOr4M6ePauGhobw501NTTp8+LAyMzM1fPhwLV26VL/61a90++23q6CgQC+++KLy8vI0a9asWM4NAEhwngN04MABPfTQQ+HPKyoqJEnz5s3Txo0b9eyzz6qjo0MLFy7UmTNndP/992v37t0aOHBg7KYGACQ8zwGaPHmynHNXvd/n8+nll1/Wyy+/fF2DAZfdd999ntdc7WX/8fDCCy/02mMBycT8VXAAgBsTAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHh+N2ygt82YMcPzmldffTWqx+rXr5/nNaWlpVE9FnCj4woIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBm5ECX9K/v/f/JQoLC+MwCZD8uAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHgO0N69ezVjxgzl5eXJ5/Np+/btEffPnz9fPp8vYps+fXqs5gUAJAnPAero6FBRUZHWrl171X2mT5+u5ubm8LZ58+brGhIAkHz6e11QVlamsrKyr93H7/crGAxGPRQAIPnF5TmgmpoaZWdn64477tDixYt1+vTpq+7b1dWlUCgUsQEAkl/MAzR9+nS9+eabqq6u1m9/+1vV1taqrKxMFy9e7HH/qqoqBQKB8Jafnx/rkQAAfZDnH8Fdy9y5c8Mfjx07VuPGjdPIkSNVU1OjKVOmXLF/ZWWlKioqwp+HQiEiBAA3gLi/DLuwsFBZWVlqaGjo8X6/36/09PSIDQCQ/OIeoE8//VSnT59Wbm5uvB8KAJBAPP8I7uzZsxFXM01NTTp8+LAyMzOVmZmpl156SXPmzFEwGFRjY6OeffZZ3XbbbZo2bVpMBwcAJDbPATpw4IAeeuih8OeXn7+ZN2+e1q1bpyNHjuhPf/qTzpw5o7y8PE2dOlW//OUv5ff7Yzc1ACDh+ZxzznqILwuFQgoEAmpra+P5IEiSzp0753nNqFGjonqszz77zPOalpYWz2uGDBnieQ2QKL7p93HeCw4AYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmYv4ruYFYGzx4sOc10f76j//+97+e14wdO9bzmmAw6HlNNBYtWhTVuh/96Eee1wwcODCqx8KNiysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEb0aKpDRlypSo1r3++uue1zQ3N/fKmmj8+Mc/jmrd7t27Pa/59a9/7XnNqFGjPK9B8uAKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwZuRIin94Q9/iGrdgw8+6HnN2LFjPa/Zv3+/5zVvvPGG5zUffvih5zWStG3bNs9r7rnnHs9rnn/+ec9rkDy4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856iC8LhUIKBAJqa2tTenq69ThAn3H27FnPa4qLi6N6rI8//tjzmokTJ3peU1NT43lNSgp/b+7rvun3cf4kAQAmCBAAwISnAFVVVenee+9VWlqasrOzNWvWLNXX10fs09nZqfLyct1yyy26+eabNWfOHLW2tsZ0aABA4vMUoNraWpWXl2vfvn3as2ePLly4oKlTp6qjoyO8z9NPP62dO3dq69atqq2t1cmTJzV79uyYDw4ASGyefiPq7t27Iz7fuHGjsrOzdfDgQU2aNEltbW16/fXXtWnTJn3/+9+XJG3YsEF33nmn9u3bp+9973uxmxwAkNCu6zmgtrY2SVJmZqYk6eDBg7pw4YJKS0vD+4wePVrDhw9XXV1dj1+jq6tLoVAoYgMAJL+oA9Td3a2lS5dq4sSJGjNmjCSppaVFqampysjIiNg3JydHLS0tPX6dqqoqBQKB8Jafnx/tSACABBJ1gMrLy3X06FFt2bLlugaorKxUW1tbeDtx4sR1fT0AQGLw9BzQZUuWLNGuXbu0d+9eDRs2LHx7MBjU+fPndebMmYiroNbWVgWDwR6/lt/vl9/vj2YMAEAC83QF5JzTkiVLtG3bNr333nsqKCiIuH/8+PEaMGCAqqurw7fV19fr+PHjKikpic3EAICk4OkKqLy8XJs2bdKOHTuUlpYWfl4nEAho0KBBCgQCevLJJ1VRUaHMzEylp6frqaeeUklJCa+AAwBE8BSgdevWSZImT54ccfuGDRs0f/58SdLvfvc7paSkaM6cOerq6tK0adP0+9//PibDAgCSB29GCiSxnTt3RrVu7ty5ntd0dnZ6XnP+/HnPa/r16+d5DXoXb0YKAOjTCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCKq34gKIDHMmDEjqnV33nmn5zWHDh2K6rFw4+IKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwZuRAkmsvb09qnX//ve/YzwJcCWugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE7wZKZDENmzYENW6Tz75xPOaCRMmeF7j8/k8r0Hy4AoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBm5ECSWzixIm99livvvqq5zUpKfwd+EbGnz4AwAQBAgCY8BSgqqoq3XvvvUpLS1N2drZmzZql+vr6iH0mT54sn88XsS1atCimQwMAEp+nANXW1qq8vFz79u3Tnj17dOHCBU2dOlUdHR0R+y1YsEDNzc3hbeXKlTEdGgCQ+Dy9CGH37t0Rn2/cuFHZ2dk6ePCgJk2aFL598ODBCgaDsZkQAJCUrus5oLa2NklSZmZmxO1vvfWWsrKyNGbMGFVWVurcuXNX/RpdXV0KhUIRGwAg+UX9Muzu7m4tXbpUEydO1JgxY8K3P/bYYxoxYoTy8vJ05MgRPffcc6qvr9c777zT49epqqrSSy+9FO0YAIAEFXWAysvLdfToUX3wwQcRty9cuDD88dixY5Wbm6spU6aosbFRI0eOvOLrVFZWqqKiIvx5KBRSfn5+tGMBABJEVAFasmSJdu3apb1792rYsGFfu29xcbEkqaGhoccA+f1++f3+aMYAACQwTwFyzumpp57Stm3bVFNTo4KCgmuuOXz4sCQpNzc3qgEBAMnJU4DKy8u1adMm7dixQ2lpaWppaZEkBQIBDRo0SI2Njdq0aZN+8IMf6JZbbtGRI0f09NNPa9KkSRo3blxc/gMAAInJU4DWrVsn6dI/Nv2yDRs2aP78+UpNTdW7776r1atXq6OjQ/n5+ZozZ45eeOGFmA0MAEgOnn8E93Xy8/NVW1t7XQMBAG4MvBs2kMTGjx8f1bqLFy/GeBLgSrwZKQDABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb6Ww/wVc45SVIoFDKeBAAQjcvfvy9/P7+aPheg9vZ2SVJ+fr7xJACA69He3q5AIHDV+33uWonqZd3d3Tp58qTS0tLk8/ki7guFQsrPz9eJEyeUnp5uNKE9jsMlHIdLOA6XcBwu6QvHwTmn9vZ25eXlKSXl6s/09LkroJSUFA0bNuxr90lPT7+hT7DLOA6XcBwu4ThcwnG4xPo4fN2Vz2W8CAEAYIIAAQBMJFSA/H6/li9fLr/fbz2KKY7DJRyHSzgOl3AcLkmk49DnXoQAALgxJNQVEAAgeRAgAIAJAgQAMEGAAAAmEiZAa9eu1a233qqBAwequLhYH374ofVIvW7FihXy+XwR2+jRo63Hiru9e/dqxowZysvLk8/n0/bt2yPud85p2bJlys3N1aBBg1RaWqpjx47ZDBtH1zoO8+fPv+L8mD59us2wcVJVVaV7771XaWlpys7O1qxZs1RfXx+xT2dnp8rLy3XLLbfo5ptv1pw5c9Ta2mo0cXx8k+MwefLkK86HRYsWGU3cs4QI0Ntvv62KigotX75cH330kYqKijRt2jSdOnXKerRed/fdd6u5uTm8ffDBB9YjxV1HR4eKioq0du3aHu9fuXKlXnvtNa1fv1779+/XTTfdpGnTpqmzs7OXJ42vax0HSZo+fXrE+bF58+ZenDD+amtrVV5ern379mnPnj26cOGCpk6dqo6OjvA+Tz/9tHbu3KmtW7eqtrZWJ0+e1OzZsw2njr1vchwkacGCBRHnw8qVK40mvgqXACZMmODKy8vDn1+8eNHl5eW5qqoqw6l63/Lly11RUZH1GKYkuW3btoU/7+7udsFg0L3yyivh286cOeP8fr/bvHmzwYS946vHwTnn5s2b52bOnGkyj5VTp045Sa62ttY5d+nPfsCAAW7r1q3hff75z386Sa6urs5qzLj76nFwzrkHH3zQ/fSnP7Ub6hvo81dA58+f18GDB1VaWhq+LSUlRaWlpaqrqzOczMaxY8eUl5enwsJCPf744zp+/Lj1SKaamprU0tIScX4EAgEVFxffkOdHTU2NsrOzdccdd2jx4sU6ffq09Uhx1dbWJknKzMyUJB08eFAXLlyIOB9Gjx6t4cOHJ/X58NXjcNlbb72lrKwsjRkzRpWVlTp37pzFeFfV596M9Ks+//xzXbx4UTk5ORG35+Tk6OOPPzaaykZxcbE2btyoO+64Q83NzXrppZf0wAMP6OjRo0pLS7Mez0RLS4sk9Xh+XL7vRjF9+nTNnj1bBQUFamxs1C9+8QuVlZWprq5O/fr1sx4v5rq7u7V06VJNnDhRY8aMkXTpfEhNTVVGRkbEvsl8PvR0HCTpscce04gRI5SXl6cjR47oueeeU319vd555x3DaSP1+QDh/8rKysIfjxs3TsXFxRoxYoT++te/6sknnzScDH3B3Llzwx+PHTtW48aN08iRI1VTU6MpU6YYThYf5eXlOnr06A3xPOjXudpxWLhwYfjjsWPHKjc3V1OmTFFjY6NGjhzZ22P2qM//CC4rK0v9+vW74lUsra2tCgaDRlP1DRkZGRo1apQaGhqsRzFz+Rzg/LhSYWGhsrKykvL8WLJkiXbt2qX3338/4te3BINBnT9/XmfOnInYP1nPh6sdh54UFxdLUp86H/p8gFJTUzV+/HhVV1eHb+vu7lZ1dbVKSkoMJ7N39uxZNTY2Kjc313oUMwUFBQoGgxHnRygU0v79+2/48+PTTz/V6dOnk+r8cM5pyZIl2rZtm9577z0VFBRE3D9+/HgNGDAg4nyor6/X8ePHk+p8uNZx6Mnhw4clqW+dD9avgvgmtmzZ4vx+v9u4caP7xz/+4RYuXOgyMjJcS0uL9Wi96mc/+5mrqalxTU1N7m9/+5srLS11WVlZ7tSpU9ajxVV7e7s7dOiQO3TokJPkVq1a5Q4dOuQ++eQT55xzv/nNb1xGRobbsWOHO3LkiJs5c6YrKChwX3zxhfHksfV1x6G9vd0988wzrq6uzjU1Nbl3333Xffe733W333676+zstB49ZhYvXuwCgYCrqalxzc3N4e3cuXPhfRYtWuSGDx/u3nvvPXfgwAFXUlLiSkpKDKeOvWsdh4aGBvfyyy+7AwcOuKamJrdjxw5XWFjoJk2aZDx5pIQIkHPOrVmzxg0fPtylpqa6CRMmuH379lmP1OseeeQRl5ub61JTU923vvUt98gjj7iGhgbrseLu/fffd5Ku2ObNm+ecu/RS7BdffNHl5OQ4v9/vpkyZ4urr622HjoOvOw7nzp1zU6dOdUOHDnUDBgxwI0aMcAsWLEi6v6T19N8vyW3YsCG8zxdffOF+8pOfuCFDhrjBgwe7hx9+2DU3N9sNHQfXOg7Hjx93kyZNcpmZmc7v97vbbrvN/fznP3dtbW22g38Fv44BAGCizz8HBABITgQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAif8BWVRoMK5yyjgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#create csv from ubyte file\n",
        "def convert(imgf, labelf, outf, n):\n",
        "    f = open(imgf, \"rb\")\n",
        "    o = open(outf, \"w\")\n",
        "    l = open(labelf, \"rb\")\n",
        "    f.read(16)\n",
        "    l.read(8)\n",
        "    images = []\n",
        "\n",
        "    for i in range(n):\n",
        "        image = [ord(l.read(1))] # first element is the label\n",
        "        for j in range(28*28): # the grey values\n",
        "            image.append(ord(f.read(1)))\n",
        "        images.append(image)\n",
        "\n",
        "    for image in images:\n",
        "        o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
        "    f.close()\n",
        "    o.close()\n",
        "    l.close()\n",
        "\n",
        "convert(\"/content/data/MNIST/raw/train-images-idx3-ubyte\", \"/content/data/MNIST/raw/train-labels-idx1-ubyte\",\n",
        "        \"/content/data/MNIST/raw/mnist_train.csv\", 60000)\n",
        "convert(\"/content/data/MNIST/raw/t10k-images-idx3-ubyte\", \"/content/data/MNIST/raw/t10k-labels-idx1-ubyte\",\n",
        "        \"/content/data/MNIST/raw/mnist_test.csv\", 10000)\n",
        "\n",
        "\n",
        "# open the CSV file and read its contents into a list\n",
        "train_file = open(\"/content/data/MNIST/raw/mnist_train.csv\", 'r')\n",
        "train_list = train_file.readlines()\n",
        "train_file.close()\n",
        "\n",
        "#len(train_list)\n",
        "\n",
        "# show a dataset record\n",
        "# the first number is the label, the rest are pixel colour values (greyscale 0-255)\n",
        "train_list[50]\n",
        "\n",
        "# take the data from a record, rearrange it into a 28*28 array and plot it as an image\n",
        "all_values = train_list[100].split(',')\n",
        "image_array = np.asfarray(all_values[1:]).reshape((28,28))\n",
        "matplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')\n",
        "\n",
        "# open the CSV file and read its contents into a list\n",
        "test_file = open(\"/content/data/MNIST/raw/mnist_test.csv\", 'r')\n",
        "test_list = test_file.readlines()\n",
        "test_file.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "_jL4JVwzSXOb",
        "outputId": "a8334ff9-48ec-4d57-805b-3846e2ce5e79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7c2d008d2830>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#check test data\n",
        "all_values = test_list[100].split(',')\n",
        "image_array = np.asfarray(all_values[1:]).reshape((28,28))\n",
        "matplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')\n",
        "\n",
        "# # standardize train data range from 0.01 to 1.00\n",
        "# scaled_input_train = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "# #print(scaled_input_train)\n",
        "\n",
        "# # scale test data to range 0.01 to 1.00\n",
        "# scaled_input_test = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "# # print(scaled_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "D6EMRpsjOyxo"
      },
      "outputs": [],
      "source": [
        "class DNN:\n",
        "  def __init__(self ,sizes ,epochs, lr, lambd = 0.1):\n",
        "    self.sizes = sizes\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr\n",
        "    self.lambd = lambd\n",
        "\n",
        "    #structure\n",
        "    input_layer = sizes[0]\n",
        "    hidden_1 = sizes[1]\n",
        "    hidden_2 = sizes[2]\n",
        "    output_layer = sizes[3]\n",
        "\n",
        "    #initial weights randomly\n",
        "    self.params = {\n",
        "        'W1': np.random.randn(hidden_1,input_layer) * np.sqrt(1.0/hidden_1), # 200 * 782\n",
        "        'W2': np.random.randn(hidden_2,hidden_1) * np.sqrt(1.0/hidden_2), # 50 * 200\n",
        "        'W3': np.random.randn(output_layer,hidden_2) * np.sqrt(1.0/output_layer) # 10 * 200\n",
        "    }\n",
        "  def relu(self,x,derivative = False):\n",
        "    if derivative:\n",
        "      return np.where(x>0.0, 1.0, 0.0)\n",
        "    return np.maximum(x,0.0)\n",
        "\n",
        "  def softmax(self, x, derivative = False):\n",
        "    exps = np.exp(x-x.max()) # for stability\n",
        "    if derivative:\n",
        "      return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "    return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def forward(self,x_train):\n",
        "    params = self.params\n",
        "\n",
        "    params['A0'] = x_train # 782 * 1\n",
        "\n",
        "    #input layer to hidden_1\n",
        "    params['Z1'] = np.dot( params['W1'], params['A0']) # 200*1\n",
        "    params['A1'] = self.relu(params['Z1'])\n",
        "\n",
        "    #hidden_1 to hidden_2\n",
        "    params['Z2'] = np.dot(params['W2'], params['A1'])\n",
        "    params['A2'] = self.relu(params['Z2'])\n",
        "\n",
        "    #hidden_2 to output_layer\n",
        "    params['Z3'] = np.dot(params['W3'], params['Z2'])\n",
        "    params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "    return params['A3']\n",
        "\n",
        "  def backward(self, y_train, output):\n",
        "    params = self.params\n",
        "    change_w = {}\n",
        "\n",
        "    #calcualate and update W3 with its gradient and l2-norm repectively\n",
        "    error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
        "      # out - y_train: diff, devided by out shape for stablility, sofltmax gradient CHAIN RULE\n",
        "    change_w['W3'] = np.outer(error, params['A2']) + self.lambd * params['W3']\n",
        "      # np.outer: outer product, distribute the shared gradients to each nuron\n",
        "\n",
        "    # W2 and update with l2-norm\n",
        "    error = np.dot(params['W3'].T, error) * self.relu(params['Z2'], derivative=True) #CHAIN RULE\n",
        "    change_w['W2'] = np.outer(error, params['A1']) + self.lambd * params['W2']\n",
        "\n",
        "    # W1 and update with l2-norm\n",
        "    error = np.dot(params['W2'].T, error) * self.relu(params['Z1'], derivative=True)\n",
        "    change_w['W1'] = np.outer(error , params['A0']) + self.lambd * params['W1']\n",
        "\n",
        "    return change_w\n",
        "\n",
        "\n",
        "  def update_weights(self, change_w):\n",
        "    for key, val in change_w.items():\n",
        "      self.params[key] -= self.lr * val\n",
        "\n",
        "  def compute_accuracy(self, test_data, output_nodes):\n",
        "      correct_predictions = 0\n",
        "      total_predictions = 0\n",
        "\n",
        "      for record in test_data:\n",
        "          all_values = record.split(',')\n",
        "          inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "          correct_label = int(all_values[0])\n",
        "          outputs = self.forward(inputs)\n",
        "          predicted_label = np.argmax(outputs)\n",
        "\n",
        "          if predicted_label == correct_label:\n",
        "              correct_predictions += 1\n",
        "          total_predictions += 1\n",
        "\n",
        "      return correct_predictions / total_predictions\n",
        "\n",
        "  def accuracy(self, test_data, output_nodes):\n",
        "      predictions = []\n",
        "\n",
        "      for x in train_list:\n",
        "          all_values = x.split(',')\n",
        "          # scale and shift the inputs\n",
        "          inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "          # create the target output values (all 0.01, except the desired label which is 0.99)\n",
        "          targets = np.zeros(output_nodes) + 0.01\n",
        "          # all_values[0] is the target label for this record\n",
        "          targets[int(all_values[0])] = 0.99\n",
        "          output = self.forward_pass(inputs)\n",
        "          pred = np.argmax(output)\n",
        "          predictions.append(pred == np.argmax(targets))\n",
        "\n",
        "\n",
        "  def train(self, train_list, test_list, output_nodes):\n",
        "      start_time = time.time()\n",
        "      for iteration in range(self.epochs):\n",
        "          for x in train_list:\n",
        "              all_values = x.split(',')\n",
        "              # scale and shift the inputs\n",
        "              inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "              # create the target output values (all 0.01, except the desired label which is 0.99)\n",
        "              targets = np.zeros(output_nodes) + 0.01\n",
        "              # all_values[0] is the target label for this record\n",
        "              targets[int(all_values[0])] = 0.99\n",
        "              output = self.forward(inputs)\n",
        "              changes_to_w = self.backward(targets, output)\n",
        "              self.update_weights(changes_to_w)\n",
        "\n",
        "          #verbose\n",
        "          accuracy = self.compute_accuracy(test_list, output_nodes)\n",
        "          print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "              iteration+1, time.time() - start_time, accuracy * 100\n",
        "          ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_suvGj1P_QU",
        "outputId": "6b71e6b9-5a0d-467c-ee8b-e92687c4ed2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 119.33s, Accuracy: 91.02%\n",
            "Epoch: 2, Time Spent: 238.19s, Accuracy: 92.08%\n",
            "Epoch: 3, Time Spent: 361.18s, Accuracy: 92.51%\n",
            "Epoch: 4, Time Spent: 482.48s, Accuracy: 93.17%\n",
            "Epoch: 5, Time Spent: 601.27s, Accuracy: 93.64%\n",
            "Epoch: 6, Time Spent: 719.46s, Accuracy: 93.54%\n"
          ]
        }
      ],
      "source": [
        "dnn = DNN(sizes=[784, 200, 50, 10], epochs=6, lr=0.0195, lambd = 0.00023)\n",
        "dnn.train(train_list, test_list, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CCeCXhz9dWJE"
      },
      "outputs": [],
      "source": [
        "# class DNN:\n",
        "#   def __init__(self,sizes = [784,200,50,10],epochs = 10, lr = 0.01, lambda_rg=0.001):\n",
        "#     self.sizes = sizes\n",
        "#     self.epochs = epochs\n",
        "#     self.lr = lr\n",
        "#     self.lambda_rg = lambda_rg\n",
        "#     #structure\n",
        "#     input_layer = sizes[0]\n",
        "#     hidden_1 = sizes[1]\n",
        "#     hidden_2 = sizes[2]\n",
        "#     output_layer = sizes[3]\n",
        "\n",
        "#     #initial weights randomly\n",
        "#     self.params = {\n",
        "#         'W1': np.random.randn(hidden_1,input_layer) * np.sqrt(1.0/hidden_1), # 200 * 782\n",
        "#         'W2': np.random.randn(hidden_2,hidden_1) * np.sqrt(1.0/hidden_2), # 50 * 200\n",
        "#         'W3': np.random.randn(output_layer,hidden_2) * np.sqrt(1.0/output_layer) # 10 * 200\n",
        "#     }\n",
        "#   def relu(self,x,derivative = False):\n",
        "#     if derivative:\n",
        "#       return np.where(x>0.0, 1.0, 0.0)\n",
        "#     return np.maximum(x,0.0)\n",
        "\n",
        "#   def softmax(self, x, derivative = False):\n",
        "#     exps = np.exp(x-x.max()) # for stability\n",
        "#     if derivative:\n",
        "#       return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "#     return exps / np.sum(exps, axis=0)\n",
        "\n",
        "#   def forward(self, x_train_batch):\n",
        "#       params = self.params\n",
        "\n",
        "#       params['A0'] = x_train_batch  # This now expects a batch of inputs\n",
        "\n",
        "#       # Input layer to hidden_1\n",
        "#       params['Z1'] = np.dot(params['W1'], params['A0'].T)  # Adjusted for batch processing\n",
        "#       params['A1'] = self.relu(params['Z1'])\n",
        "\n",
        "#       # Hidden_1 to hidden_2\n",
        "#       params['Z2'] = np.dot(params['W2'], params['A1'])\n",
        "#       params['A2'] = self.relu(params['Z2'])\n",
        "\n",
        "#       # Hidden_2 to output_layer\n",
        "#       params['Z3'] = np.dot(params['W3'], params['A2'])\n",
        "#       params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "#       return params['A3'].T  # Transpose back for batch processing\n",
        "\n",
        "\n",
        "#   # def backward(self, y_train, output):\n",
        "#   #   params = self.params\n",
        "#   #   change_w = {}\n",
        "\n",
        "#   #   #calcualate and update W3 with its gradient repectively\n",
        "#   #   error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True) # out - y_train: diff, devided by out shape for stablility, sofltmax gradient CHAIN RULE\n",
        "#   #   change_w['W3'] = np.outer(error, params['A2']) # np.outer: outer product, distribute the shared gradients to each nuron\n",
        "\n",
        "#   #   # W2 and update\n",
        "#   #   error = np.dot(params['W3'].T, error) * self.relu(params['Z2'], derivative=True) #CHAIN RULE\n",
        "#   #   change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "#   #   # W1 and update\n",
        "#   #   error = np.dot(params['W2'].T, error) * self.relu(params['Z1'], derivative=True)\n",
        "#   #   change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "#   #   return change_w\n",
        "\n",
        "\n",
        "#   def backward(self, y_train_batch, output_batch):\n",
        "#       params = self.params\n",
        "#       batch_size = y_train_batch.shape[0]\n",
        "\n",
        "#       # Initialize gradient accumulation variables\n",
        "#       grad_accum_W1 = np.zeros_like(self.params['W1'])\n",
        "#       grad_accum_W2 = np.zeros_like(self.params['W2'])\n",
        "#       grad_accum_W3 = np.zeros_like(self.params['W3'])\n",
        "\n",
        "#       for i in range(batch_size):\n",
        "#           # Extract the i-th training example and output\n",
        "#           y_train = y_train_batch[i, :]\n",
        "#           output = output_batch[i, :]\n",
        "\n",
        "#           # Calculate and update W3 gradient\n",
        "#           error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'][:, i], derivative=True)\n",
        "#           grad_accum_W3 += np.outer(error, params['A2'][:, i])\n",
        "\n",
        "#           # W2 gradient\n",
        "#           error = np.dot(params['W3'].T, error) * self.relu(params['Z2'][:, i], derivative=True)\n",
        "#           grad_accum_W2 += np.outer(error, params['A1'][:, i])\n",
        "\n",
        "#           # W1 gradient\n",
        "#           error = np.dot(params['W2'].T, error) * self.relu(params['Z1'][:, i], derivative=True)\n",
        "#           grad_accum_W1 += np.outer(error, params['A0'][i, :])\n",
        "\n",
        "#       # Average the accumulated gradients\n",
        "#       grad_W1 = (grad_accum_W1 / batch_size) + (self.lambda_rg * self.params['W1'])\n",
        "#       grad_W2 = (grad_accum_W2 / batch_size) + (self.lambda_rg * self.params['W2'])\n",
        "#       grad_W3 = (grad_accum_W3 / batch_size) + (self.lambda_rg * self.params['W3'])\n",
        "\n",
        "#       return {'W1': grad_W1, 'W2': grad_W2, 'W3': grad_W3}\n",
        "\n",
        "\n",
        "\n",
        "#   def update_weights(self, change_w):\n",
        "#     for key, val in change_w.items():\n",
        "#       self.params[key] -= self.lr * val\n",
        "\n",
        "#   def compute_accuracy(self, test_data, output_nodes):\n",
        "#       correct_predictions = 0\n",
        "#       total_predictions = 0\n",
        "\n",
        "#       for record in test_data:\n",
        "#           all_values = record.split(',')\n",
        "#           inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "#           correct_label = int(all_values[0])\n",
        "#           outputs = self.forward(inputs)\n",
        "#           predicted_label = np.argmax(outputs)\n",
        "\n",
        "#           if predicted_label == correct_label:\n",
        "#               correct_predictions += 1\n",
        "#           total_predictions += 1\n",
        "\n",
        "#       return correct_predictions / total_predictions\n",
        "\n",
        "#   def train(self, train_list, test_list, output_nodes):\n",
        "#       mini_batch_size = 512\n",
        "#       n = len(train_list)\n",
        "#       start_time = time.time()\n",
        "\n",
        "#       for i in range(self.epochs):\n",
        "#           np.random.shuffle(train_list)\n",
        "#           mini_batches = [\n",
        "#               train_list[k:k+mini_batch_size]\n",
        "#               for k in range(0, n, mini_batch_size)\n",
        "#           ]\n",
        "\n",
        "#           for mini_batch in mini_batches:\n",
        "#               grad_W1 = np.zeros_like(self.params['W1'])\n",
        "#               grad_W2 = np.zeros_like(self.params['W2'])\n",
        "#               grad_W3 = np.zeros_like(self.params['W3'])\n",
        "\n",
        "#               for x in mini_batch:\n",
        "#                   all_values = x.split(',')\n",
        "#                   inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
        "#                   targets = np.zeros(output_nodes) + 0.01\n",
        "#                   targets[int(all_values[0])] = 0.99\n",
        "\n",
        "#                   # Batch processing - adjust inputs and targets to be batch-compatible\n",
        "#                   inputs_batch = np.array([inputs])  # Make it a batch of one for now\n",
        "#                   targets_batch = np.array([targets])\n",
        "\n",
        "#                   output_batch = self.forward(inputs_batch)\n",
        "#                   changes_to_w = self.backward(targets_batch, output_batch)\n",
        "\n",
        "#                   # Accumulate gradients\n",
        "#                   grad_W1 += changes_to_w['W1']\n",
        "#                   grad_W2 += changes_to_w['W2']\n",
        "#                   grad_W3 += changes_to_w['W3']\n",
        "\n",
        "#               # Update weights\n",
        "#               batch_weights = {'W1': grad_W1 / mini_batch_size,\n",
        "#                                'W2': grad_W2 / mini_batch_size,\n",
        "#                                'W3': grad_W3 / mini_batch_size}\n",
        "#               self.update_weights(batch_weights)\n",
        "\n",
        "\n",
        "#           # Verbose\n",
        "#           accuracy = self.compute_accuracy(test_list, output_nodes)\n",
        "#           print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "#               i + 1, time.time() - start_time, accuracy * 100\n",
        "#           ))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boo9T7kHd1AR",
        "outputId": "1144da64-79af-4431-ca07-5041b723f09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 162.68s, Accuracy: 17.02%\n",
            "Epoch: 2, Time Spent: 312.87s, Accuracy: 22.00%\n",
            "Epoch: 3, Time Spent: 465.63s, Accuracy: 26.64%\n",
            "Epoch: 4, Time Spent: 613.88s, Accuracy: 30.76%\n",
            "Epoch: 5, Time Spent: 751.21s, Accuracy: 33.92%\n",
            "Epoch: 6, Time Spent: 891.48s, Accuracy: 36.65%\n",
            "Epoch: 7, Time Spent: 1026.86s, Accuracy: 39.46%\n",
            "Epoch: 8, Time Spent: 1168.19s, Accuracy: 41.83%\n",
            "Epoch: 9, Time Spent: 1310.49s, Accuracy: 44.22%\n",
            "Epoch: 10, Time Spent: 1448.52s, Accuracy: 46.05%\n"
          ]
        }
      ],
      "source": [
        "# dnn = DNN(sizes=[784, 200, 50, 10], epochs=10, lr=0.011, lambda_rg = 0.0001)\n",
        "# dnn.train(train_list, test_list, 10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}