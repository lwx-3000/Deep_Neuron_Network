{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSczdesY7/8fqXgInj307k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lwx-3000/Deep_Neuron_Network/blob/main/Repro_Sampling_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37eeXZejBIY9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import scipy.stats as stats\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQeoPb_Bi39R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba6d558-35b3-4ff9-f014-359b49ee6b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2692,  0.9548,  1.7300],\n",
            "        [ 0.4241, -0.1345, -2.5557],\n",
            "        [ 0.2105,  0.3175,  0.2257],\n",
            "        ...,\n",
            "        [ 0.7230, -1.2820, -0.6629],\n",
            "        [-0.2402, -0.3636,  0.9041],\n",
            "        [ 1.8342, -1.0857,  1.0614]]) tensor([[-0.9711],\n",
            "        [-0.1334],\n",
            "        [ 0.5546],\n",
            "        ...,\n",
            "        [-2.2126],\n",
            "        [-1.1680],\n",
            "        [-0.0612]])\n",
            "X dimsension: torch.Size([10000, 3]), y_true.shape: torch.Size([10000, 1])\n"
          ]
        }
      ],
      "source": [
        "#Section 1, Data generation, define NN model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, std_deviation):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        previous_size = input_size\n",
        "        self.std_dev = abs(std_deviation)\n",
        "        for layer_size in hidden_layers:\n",
        "            self.hidden_layers.append(nn.Linear(previous_size, layer_size))\n",
        "            previous_size = layer_size\n",
        "        self.output_layer = nn.Linear(previous_size, output_size)\n",
        "\n",
        "    def forward(self, x, add_noise = True):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        if add_noise: # add noise to each output. fix dimensionality problem\n",
        "            noise = torch.randn_like(x) * self.std_dev #save noise?\n",
        "            x = x + noise\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "input_size, hidden_layers, output_size = 3, [3, 3], 1  # L = 2 K = 3 [# make L, K as input parameter]\n",
        "std_dev = 1.3 #true standard deviation of noise\n",
        "\n",
        "model = SimpleNN(input_size, hidden_layers, output_size, std_dev)\n",
        "\n",
        "# Manually set the weights and biases for each neuron\n",
        "#layer 1\n",
        "model.hidden_layers[0].weight.data.fill_(0.12)\n",
        "model.hidden_layers[0].bias.data.fill_(0.2)\n",
        "#layer 2\n",
        "model.hidden_layers[1].weight.data.fill_(0.11)\n",
        "model.hidden_layers[1].bias.data.fill_(0.12)\n",
        "#output layer\n",
        "model.output_layer.weight.data.fill_(0.13)\n",
        "model.output_layer.bias.data.fill_(0.001)\n",
        "\n",
        "#Generate 'X' data\n",
        "n_samples = 10000 #temp\n",
        "X = torch.randn(n_samples, input_size)  #normal distributed\n",
        "\n",
        "#Pass 'X' through the model to generate 'y_true' as outputs\n",
        "with torch.no_grad():\n",
        "    y_true = model(X)\n",
        "\n",
        "#sanity check point 1\n",
        "print(X, y_true)\n",
        "print(f\"X dimsension: {X.shape}, y_true.shape: {y_true.shape}\")\n",
        "\n",
        "\n",
        "# split data into Da and Db\n",
        "# randomlize the order ? Yes\n",
        "n_b = n_samples // 2\n",
        "n_a = n_samples - n_b\n",
        "\n",
        "dataset = TensorDataset(X, y_true)\n",
        "Da, Db = random_split(dataset, [n_a, n_b])\n",
        "\n",
        "#y-output dimension?  The dimension problem of y is fixed properly displayed as 1.\n",
        "train_loader = DataLoader(Db, batch_size=100, shuffle=True)\n",
        "val_loader = DataLoader(Da, batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        previous_size = input_size\n",
        "        for layer_size in hidden_layers:\n",
        "            self.hidden_layers.append(nn.Linear(previous_size, layer_size))\n",
        "            previous_size = layer_size\n",
        "        self.output_layer = nn.Linear(previous_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = torch.relu(layer(x)) #?use a different activation?\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "eLjfJKJAhWfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training, Grid Search with residual and standard deviation packed\n",
        "class SearchNN(SimpleNN):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, lr=0.001):\n",
        "        super(SearchNN, self).__init__(input_size, hidden_layers, output_size)\n",
        "        self.loss_function = nn.MSELoss() #temperary: MSE\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        self.y_pred = []\n",
        "\n",
        "    def train_single_epoch(self, data_loader):\n",
        "        self.train()\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            self.optimizer.zero_grad()\n",
        "            y_pred = self(x_batch)\n",
        "            loss = self.loss_function(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def validate(self, data_loader):\n",
        "      self.eval()  # Set the model to evaluation mode\n",
        "      val_losses = []\n",
        "      all_residuals = []  # Use a list to collect all residuals\n",
        "\n",
        "      with torch.no_grad():  # No gradient needed for validation\n",
        "          for x_val, y_val in data_loader:\n",
        "              y_val_pred = self(x_val) #this need to be output for data repro\n",
        "              val_loss = self.loss_function(y_val_pred, y_val)\n",
        "              val_losses.append(val_loss.item())\n",
        "\n",
        "              # Calculate residuals and predicted y and append them to the list\n",
        "              residuals = y_val - y_val_pred\n",
        "              all_residuals.append(residuals)\n",
        "              self.y_pred.append(y_val_pred) #need to call .item()? is y_pred a tensor? No\n",
        "\n",
        "      # Concatenate all residuals tensors to form a single tensor\n",
        "      all_residuals_tensor = torch.cat(all_residuals, dim=0)\n",
        "\n",
        "      # Concatenate all predicted_y value\n",
        "      y_pred_tensor = torch.cat(self.y_pred, dim=0)\n",
        "\n",
        "      # Calculate the standard deviation of residuals\n",
        "      # Standard deviation of validate errors.2\n",
        "      standard_deviation = torch.sqrt(torch.mean(all_residuals_tensor ** 2))\n",
        "\n",
        "      # The mean validation loss\n",
        "      mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "\n",
        "      # Save the residuals and standard deviation as attributes for later use\n",
        "      self.residuals = all_residuals_tensor\n",
        "      self.standard_deviation = standard_deviation\n",
        "\n",
        "      # add parameter save feature\n",
        "\n",
        "      return mean_val_loss\n",
        "\n",
        "\n",
        "# Repro with fixed parameter\n",
        "class Repro():\n",
        "  def train_and_evaluate_nn(self, grid_search_param, train_loader, val_loader, num_epochs=10, gradient_fix = False): #randomize epochs?\n",
        "      results = []\n",
        "      L, K = grid_search_param\n",
        "      # Model with L layers and K units\n",
        "      model = SearchNN(input_size, [K] * L, output_size)\n",
        "\n",
        "      if not gradient_fix: #functionality of validate, keep trained parameters\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = model.train_single_epoch(train_loader) # change to validate\n",
        "            #accuracy =\n",
        "      val_loss = model.validate(val_loader)\n",
        "      residual = model.residuals\n",
        "      standard_dev = model.standard_deviation\n",
        "      size = model.residuals.numel()\n",
        "      self.y_pred = model.y_pred #y predicted\n",
        "      self.est_std_dev = torch.cat(self.y_pred,dim=0).std().item()\n",
        "\n",
        "      # Define the nuclear mapping function given (L,K) on Da:\n",
        "      # l(L,K) = -n * log(sqrt(2*pi)*std_dev) - sum(residual**2) / 2*std_dev**2\n",
        "      likelihood = -size * np.log(np.sqrt(2*np.pi)*standard_dev) - np.sum(torch.pow(residual,2).tolist())/ 2*std_dev**2\n",
        "\n",
        "\n",
        "      # results\n",
        "      result = {\n",
        "          'L': L,\n",
        "          'K': K,\n",
        "          'val_loss': val_loss,\n",
        "          'residual': residual,\n",
        "          'standard_dev': standard_dev,\n",
        "          'log_likelihood': likelihood\n",
        "\n",
        "      }\n",
        "      if not gradient_fix:\n",
        "        result['train_loss'] = train_loss\n",
        "      return result\n",
        "\n",
        "# prepare Repro_data\n",
        "  def repro_data(self, data, repro_quantity, est_std_dev, seed=39):\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    self.noise_std = est_std_dev\n",
        "    self.original_data = data\n",
        "    self.noise = {}\n",
        "    self.newdata_cache = {}\n",
        "\n",
        "    if isinstance(data, torch.utils.data.Subset):\n",
        "        # Accessing the underlying dataset and indices\n",
        "        original_dataset = data.dataset\n",
        "        indices = data.indices\n",
        "        original_data_tensors = [tensor[indices] for tensor in original_dataset.tensors]\n",
        "    elif isinstance(data, torch.utils.data.TensorDataset):\n",
        "        # Directly accessing the tensors\n",
        "        original_data_tensors = data.tensors\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported dataset type\")\n",
        "\n",
        "\n",
        "    for repro_index in range(repro_quantity):\n",
        "        # Generating noise for each tensor in the dataset\n",
        "        noise_tensors = tuple(self.noise_std * torch.randn_like(tensor) for tensor in original_data_tensors)\n",
        "        self.noise[repro_index] = noise_tensors\n",
        "\n",
        "        # Adding noise to the original data tensors\n",
        "        synthetic_data_tensors = tuple(tensor + noise for tensor, noise in zip(original_data_tensors, noise_tensors))\n",
        "        self.newdata_cache[repro_index] = TensorDataset(*synthetic_data_tensors)\n",
        "\n",
        "    return self.newdata_cache\n",
        "\n",
        "# do not train\n",
        "  def monte_carlo(self,data,n_repro,l_low=1,l_up=4,k_low=3,k_up=6,epoch=10,verbose=True):\n",
        "    self.grid_search_params = [(l,k) for l in range(l_low, l_up) for k in range(k_low,k_up)]\n",
        "    self.n_epoch = epoch\n",
        "    self.n_repro = n_repro\n",
        "    self.repro_results = {}\n",
        "\n",
        "    for param in self.grid_search_params:\n",
        "      self.train_and_evaluate_nn(param, train_loader, val_loader, self.n_repro, gradient_fix = False) # obtain trained model fDb, y_pred and est_std_dev\n",
        "      data_cache = self.repro_data(data,n_repro,self.est_std_dev) #generate repro data\n",
        "      for repro_index, synthetic_data in data_cache.items():\n",
        "        repro_train_loader = DataLoader(synthetic_data, batch_size=100, shuffle=True) # do not train, use validate\n",
        "        # Do not train, use previous trained model\n",
        "        self.repro_results[(param,repro_index)] = (repro_index, self.train_and_evaluate_nn(param, repro_train_loader, val_loader, self.n_epoch, gradient_fix = True))\n",
        "\n",
        "    for (l, k), repro_result_tuple in self.repro_results.items():\n",
        "        repro_index, result = repro_result_tuple\n",
        "        #train_loss = result['train_loss']\n",
        "        val_loss = result['val_loss']\n",
        "        residual = result['residual']\n",
        "        standard_dev = result['standard_dev']\n",
        "        likelihood = result['log_likelihood']\n",
        "        if verbose:\n",
        "            print(f\"Architecture:{l}, Repro Index:{repro_index},  Val Loss: {val_loss}, Std Dev: {standard_dev}, Log Likelihood: {likelihood}\")\n",
        "    return self.repro_results\n",
        "\n",
        "  def borel_confidence_interval(self, data, alpha = 0.95): # is borel set a confidence interval or a set?\n",
        "    self.data = np.array(data)\n",
        "    self.mean = np.mean(data)\n",
        "    self.n = len(data)\n",
        "    self.std_dev = np.std(data, ddof=1)\n",
        "    self.z_score = stats.norm.ppf(1 - (1-alpha)/2) # for over 30, distribution is approx Normal\n",
        "    self.margin_of_error = self.z_score * (std_dev / np.sqrt(self.n))\n",
        "    self.confidence_interval = (self.mean - self.margin_of_error, self.mean + self.margin_of_error)\n",
        "    return self.confidence_interval\n",
        "\n",
        "# val_loader require initialization\n"
      ],
      "metadata": {
        "id": "lq81A-9Ar9oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_repro=5\n",
        "myrepro = Repro() # contains training\n",
        "# generate Monte-calo Data Da* and obtain Borel data\n",
        "repro_result = myrepro.monte_carlo(Db, n_repro=5, l_low=1,l_up=4,k_low=3,k_up=6,verbose=False)\n",
        "print(myrepro.est_std_dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96vnU7r84Fz4",
        "outputId": "b3afe524-8711-4035-c6b5-1341246db3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:(1, 3), Repro Index:0,  Val Loss: 1.9530778336524963, Std Dev: 1.397525668144226, Log Likelihood: -14519.962890625\n",
            "Architecture:(1, 3), Repro Index:1,  Val Loss: 1.779942398071289, Std Dev: 1.334144949913025, Log Likelihood: -13556.40234375\n",
            "Architecture:(1, 3), Repro Index:2,  Val Loss: 1.723296914100647, Std Dev: 1.3127440214157104, Log Likelihood: -13236.220703125\n",
            "Architecture:(1, 3), Repro Index:3,  Val Loss: 1.9893915915489198, Std Dev: 1.4104578495025635, Log Likelihood: -14719.4443359375\n",
            "Architecture:(1, 3), Repro Index:4,  Val Loss: 1.997310013771057, Std Dev: 1.4132622480392456, Log Likelihood: -14762.8310546875\n",
            "Architecture:(1, 4), Repro Index:0,  Val Loss: 1.804487748146057, Std Dev: 1.3433122634887695, Log Likelihood: -13694.345703125\n",
            "Architecture:(1, 4), Repro Index:1,  Val Loss: 1.905134449005127, Std Dev: 1.3802660703659058, Log Likelihood: -14255.267578125\n",
            "Architecture:(1, 4), Repro Index:2,  Val Loss: 1.7336917471885682, Std Dev: 1.316697359085083, Log Likelihood: -13295.173828125\n",
            "Architecture:(1, 4), Repro Index:3,  Val Loss: 1.7992673420906067, Std Dev: 1.3413676023483276, Log Likelihood: -13665.044921875\n",
            "Architecture:(1, 4), Repro Index:4,  Val Loss: 1.7142208695411683, Std Dev: 1.3092825412750244, Log Likelihood: -13184.671875\n",
            "Architecture:(1, 5), Repro Index:0,  Val Loss: 1.7306057024002075, Std Dev: 1.315524935722351, Log Likelihood: -13277.6806640625\n",
            "Architecture:(1, 5), Repro Index:1,  Val Loss: 1.7108261823654174, Std Dev: 1.307985544204712, Log Likelihood: -13165.375\n",
            "Architecture:(1, 5), Repro Index:2,  Val Loss: 1.903199610710144, Std Dev: 1.3795650005340576, Log Likelihood: -14244.552734375\n",
            "Architecture:(1, 5), Repro Index:3,  Val Loss: 1.7304448008537292, Std Dev: 1.3154637813568115, Log Likelihood: -13276.7685546875\n",
            "Architecture:(1, 5), Repro Index:4,  Val Loss: 1.887892463207245, Std Dev: 1.374005913734436, Log Likelihood: -14159.69140625\n",
            "Architecture:(2, 3), Repro Index:0,  Val Loss: 1.7544428706169128, Std Dev: 1.3245538473129272, Log Likelihood: -13412.591796875\n",
            "Architecture:(2, 3), Repro Index:1,  Val Loss: 1.7728399658203124, Std Dev: 1.3314803838729858, Log Likelihood: -13516.3984375\n",
            "Architecture:(2, 3), Repro Index:2,  Val Loss: 1.9583841514587403, Std Dev: 1.3994227647781372, Log Likelihood: -14549.1650390625\n",
            "Architecture:(2, 3), Repro Index:3,  Val Loss: 1.826703324317932, Std Dev: 1.3515559434890747, Log Likelihood: -13818.796875\n",
            "Architecture:(2, 3), Repro Index:4,  Val Loss: 1.7379101872444154, Std Dev: 1.31829833984375, Log Likelihood: -13319.072265625\n",
            "Architecture:(2, 4), Repro Index:0,  Val Loss: 2.3406031560897826, Std Dev: 1.5299030542373657, Log Likelihood: -16609.763671875\n",
            "Architecture:(2, 4), Repro Index:1,  Val Loss: 1.70148845911026, Std Dev: 1.3044110536575317, Log Likelihood: -13112.2392578125\n",
            "Architecture:(2, 4), Repro Index:2,  Val Loss: 1.8152276849746705, Std Dev: 1.347303867340088, Log Likelihood: -13754.556640625\n",
            "Architecture:(2, 4), Repro Index:3,  Val Loss: 1.9199199724197387, Std Dev: 1.3856117725372314, Log Likelihood: -14337.0634765625\n",
            "Architecture:(2, 4), Repro Index:4,  Val Loss: 1.8025995373725892, Std Dev: 1.342609167098999, Log Likelihood: -13683.75\n",
            "Architecture:(2, 5), Repro Index:0,  Val Loss: 1.697423162460327, Std Dev: 1.302851915359497, Log Likelihood: -13089.083984375\n",
            "Architecture:(2, 5), Repro Index:1,  Val Loss: 2.096674332618713, Std Dev: 1.4479897022247314, Log Likelihood: -15304.0234375\n",
            "Architecture:(2, 5), Repro Index:2,  Val Loss: 1.7740914034843445, Std Dev: 1.3319501876831055, Log Likelihood: -13523.44921875\n",
            "Architecture:(2, 5), Repro Index:3,  Val Loss: 1.7434244275093078, Std Dev: 1.3203879594802856, Log Likelihood: -13350.2890625\n",
            "Architecture:(2, 5), Repro Index:4,  Val Loss: 1.8079170346260072, Std Dev: 1.3445881605148315, Log Likelihood: -13713.58203125\n",
            "Architecture:(3, 3), Repro Index:0,  Val Loss: 1.896404869556427, Std Dev: 1.3771001100540161, Log Likelihood: -14206.90234375\n",
            "Architecture:(3, 3), Repro Index:1,  Val Loss: 1.945423696041107, Std Dev: 1.3947845697402954, Log Likelihood: -14477.80859375\n",
            "Architecture:(3, 3), Repro Index:2,  Val Loss: 1.7558010578155518, Std Dev: 1.3250664472579956, Log Likelihood: -13420.265625\n",
            "Architecture:(3, 3), Repro Index:3,  Val Loss: 1.700451786518097, Std Dev: 1.3040138483047485, Log Likelihood: -13106.3369140625\n",
            "Architecture:(3, 3), Repro Index:4,  Val Loss: 1.6879969453811645, Std Dev: 1.299229383468628, Log Likelihood: -13035.3359375\n",
            "Architecture:(3, 4), Repro Index:0,  Val Loss: 1.7117122769355775, Std Dev: 1.3083242177963257, Log Likelihood: -13170.412109375\n",
            "Architecture:(3, 4), Repro Index:1,  Val Loss: 1.7166854524612427, Std Dev: 1.3102233409881592, Log Likelihood: -13198.6767578125\n",
            "Architecture:(3, 4), Repro Index:2,  Val Loss: 1.7425957345962524, Std Dev: 1.3200740814208984, Log Likelihood: -13345.599609375\n",
            "Architecture:(3, 4), Repro Index:3,  Val Loss: 1.7402613401412963, Std Dev: 1.3191896677017212, Log Likelihood: -13332.384765625\n",
            "Architecture:(3, 4), Repro Index:4,  Val Loss: 1.9428400373458863, Std Dev: 1.3938579559326172, Log Likelihood: -14463.568359375\n",
            "Architecture:(3, 5), Repro Index:0,  Val Loss: 1.761183078289032, Std Dev: 1.3270957469940186, Log Likelihood: -13450.65625\n",
            "Architecture:(3, 5), Repro Index:1,  Val Loss: 1.801126585006714, Std Dev: 1.3420605659484863, Log Likelihood: -13675.484375\n",
            "Architecture:(3, 5), Repro Index:2,  Val Loss: 1.7889833068847656, Std Dev: 1.3375288248062134, Log Likelihood: -13607.265625\n",
            "Architecture:(3, 5), Repro Index:3,  Val Loss: 1.7453689885139465, Std Dev: 1.3211241960525513, Log Likelihood: -13361.2919921875\n",
            "Architecture:(3, 5), Repro Index:4,  Val Loss: 1.730454773902893, Std Dev: 1.3154674768447876, Log Likelihood: -13276.8251953125\n",
            "0.0022820488084107637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Empirical Distribution of Likelihood\n",
        "likelihood_distribution = {}\n",
        "lt = []\n",
        "for (l, k), repro_result_tuple in repro_result.items():\n",
        "    # temp = []\n",
        "    repro_index, result = repro_result_tuple\n",
        "    lt.append(result['log_likelihood'].numpy())\n",
        "    if repro_index == n_repro-1:\n",
        "        likelihood_distribution[l] = tuple(lt)\n",
        "        lt.clear()"
      ],
      "metadata": {
        "id": "0M-F3ydqmQOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "borel_intervals = {}\n",
        "for archtect, likelihoods in likelihood_distribution.items():\n",
        "    data = list(likelihoods)\n",
        "    interval = myrepro.borel_confidence_interval(data)\n",
        "    borel_intervals[archtect] = interval\n",
        "\n",
        "# Borel Set (intervals)\n",
        "print(borel_intervals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmWrhvGi4BEo",
        "outputId": "ace02e67-2c62-4552-b7f3-04db76f4ae71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 3): (-14160.11115899025, -14157.83220038475), (1, 4): (-13620.04084649025, -13617.76188788475), (1, 5): (-13625.95393242775, -13623.67497382225), (2, 3): (-13724.34455742775, -13722.06559882225), (2, 4): (-14300.61408867775, -14298.33513007225), (2, 5): (-13797.22346367775, -13794.94450507225), (3, 3): (-13650.46955742775, -13648.19059882225), (3, 4): (-13503.26740899025, -13500.98845038475), (3, 5): (-13475.44416680275, -13473.16520819725)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVIax378XcYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n10Khbgl2Ya-"
      },
      "outputs": [],
      "source": []
    }
  ]
}