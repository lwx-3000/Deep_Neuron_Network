{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPbbjkRmwSB/2IW9ixt/Msi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "37eeXZejBIY9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import scipy.stats as stats\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "lQeoPb_Bi39R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b28e96-61c2-412b-e62e-c986397bb90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0672,  0.0750, -0.0258],\n",
            "        [-1.3002,  0.5428, -0.1180],\n",
            "        [ 2.1826,  1.2858, -1.7478],\n",
            "        ...,\n",
            "        [-2.3269, -0.1246, -0.1468],\n",
            "        [-0.1641, -1.5404, -1.2181],\n",
            "        [ 0.2371, -0.9498,  1.8402]]) tensor([[-1.6092, -2.9530,  3.1714],\n",
            "        [ 2.1529,  2.1953, -1.1351],\n",
            "        [ 0.5223,  1.0914,  0.3954],\n",
            "        ...,\n",
            "        [-0.0418,  0.2147, -1.1560],\n",
            "        [ 1.0544,  0.2339,  1.0190],\n",
            "        [ 0.6998,  0.9911, -0.2495]])\n",
            "X dimsension: torch.Size([10000, 3]), y_true.shape: torch.Size([10000, 3])\n"
          ]
        }
      ],
      "source": [
        "#Section 1, Data generation, define NN model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, std_deviation):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        previous_size = input_size\n",
        "        self.std_dev = abs(std_deviation)\n",
        "        for layer_size in hidden_layers:\n",
        "            self.hidden_layers.append(nn.Linear(previous_size, layer_size))\n",
        "            previous_size = layer_size\n",
        "        self.output_layer = nn.Linear(previous_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, add_noise = True):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = torch.relu(layer(x))\n",
        "        if add_noise == True: # add noise to each output\n",
        "            noise = torch.randn_like(x) * self.std_dev\n",
        "        return self.output_layer(x) + noise\n",
        "\n",
        "\n",
        "input_size, hidden_layers, output_size = 3, [3, 3], 1  # L = 2 K = 3, is the \"output_size = 1\" an architextrue pram of interest?\n",
        "std_dev = 1.3 #true standard deviation of noise\n",
        "\n",
        "model = SimpleNN(input_size, hidden_layers, output_size, std_dev)\n",
        "\n",
        "# Manually set the weights and biases for each neuron\n",
        "#layer 1\n",
        "model.hidden_layers[0].weight.data.fill_(0.12)\n",
        "model.hidden_layers[0].bias.data.fill_(0.2)\n",
        "#layer 2\n",
        "model.hidden_layers[1].weight.data.fill_(0.11)\n",
        "model.hidden_layers[1].bias.data.fill_(0.12)\n",
        "#output layer\n",
        "model.output_layer.weight.data.fill_(0.13)\n",
        "model.output_layer.bias.data.fill_(0.001)\n",
        "\n",
        "#Generate 'X' data\n",
        "n_samples = 10000 #temp\n",
        "X = torch.randn(n_samples, input_size)  #normal distributed\n",
        "\n",
        "#Pass 'X' through the model to generate 'y_true' as outputs\n",
        "with torch.no_grad():\n",
        "    y_true = model(X)\n",
        "\n",
        "#sanity check point 1\n",
        "print(X, y_true) #How can I perform check without a prediction task ?\n",
        "print(f\"X dimsension: {X.shape}, y_true.shape: {y_true.shape}\")\n",
        "# split data into Da and Db\n",
        "n_b = n_samples // 2\n",
        "n_a = n_samples - n_b\n",
        "Db = TensorDataset(X[:n_b], y_true[:n_b])\n",
        "Da = TensorDataset(X[n_b:], y_true[n_b:])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(Db, batch_size=100, shuffle=True)\n",
        "val_loader = DataLoader(Da, batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        previous_size = input_size\n",
        "        for layer_size in hidden_layers:\n",
        "            self.hidden_layers.append(nn.Linear(previous_size, layer_size))\n",
        "            previous_size = layer_size\n",
        "        self.output_layer = nn.Linear(previous_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = torch.relu(layer(x))\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "eLjfJKJAhWfV"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "uKoLhGCg9dht"
      },
      "outputs": [],
      "source": [
        "# # section 2.1, Training, Grid Search\n",
        "\n",
        "# # NN for training with simple MSELoss #How do we know the original model\n",
        "# class SearchNN(SimpleNN):\n",
        "#     def __init__(self, input_size, hidden_layers, output_size, lr=0.001):\n",
        "#         super(SearchNN, self).__init__(input_size, hidden_layers, output_size)\n",
        "#         self.loss_function = nn.MSELoss() #temperary: MSE\n",
        "#         self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "#     def train_single_epoch(self, data_loader):\n",
        "#         self.train()\n",
        "#         for x_batch, y_batch in data_loader:\n",
        "#             self.optimizer.zero_grad()\n",
        "#             y_pred = self(x_batch)\n",
        "#             loss = self.loss_function(y_pred, y_batch)\n",
        "#             loss.backward()\n",
        "#             self.optimizer.step()\n",
        "#         return loss.item()\n",
        "\n",
        "#     def validate(self, data_loader):\n",
        "#         self.eval()\n",
        "#         val_losses = []\n",
        "#         y_test_pred = []\n",
        "#         residuals = []\n",
        "#         with torch.no_grad():\n",
        "#             for x_val, y_val in data_loader:\n",
        "#                 y_val_pred = self(x_val)\n",
        "#                 val_loss = self.loss_function(y_val_pred, y_val)\n",
        "#                 val_losses.append(val_loss.item())\n",
        "\n",
        "#                 y_val_pred_np = y_val_pred.detach().cpu().numpy()\n",
        "#                 y_val_np = y_val.cpu().numpy()\n",
        "\n",
        "#                 y_test_pred.extend(y_val_pred_np)\n",
        "#                 residuals.extend(y_val_np - y_val_pred_np)\n",
        "\n",
        "\n",
        "#         return sum(val_losses) / len(val_losses)\n",
        "\n",
        "# # Function: grid search, training and evaluation\n",
        "# def train_and_evaluate_nn(grid_search_params, train_loader, val_loader, num_epochs):\n",
        "#     results = []\n",
        "#     L, K = grid_search_params[0], grid_search_params[1]\n",
        "#     #Model with L layers and K units\n",
        "#     model = SearchNN(input_size, [K] * L, output_size)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss = model.train_single_epoch(train_loader)\n",
        "#     val_loss = model.validate(val_loader)\n",
        "\n",
        "#     #results\n",
        "#     results.append({\n",
        "#         'L': L,\n",
        "#         'K': K,\n",
        "#         'train_loss': train_loss,\n",
        "#         'val_loss': val_loss\n",
        "#     })\n",
        "#     return results\n",
        "\n",
        "# #Grid search\n",
        "# grid_search_params = [(l, k) for l in range(1, 4) for k in range(3, 6)] # Small 3x3 grid search\n",
        "# num_epochs = 5  # temperary\n",
        "# evaluation_results = {}\n",
        "# for (l,k) in grid_search_params:\n",
        "#    evaluation_results[(l,k)] = train_and_evaluate_nn((l,k), train_loader, val_loader, num_epochs)\n",
        "\n",
        "# print(evaluation_results.items())\n",
        "# for (L,K), result_list in evaluation_results.items():\n",
        "#   for result in result_list:\n",
        "#         train_loss = result['train_loss']\n",
        "#         val_loss = result['val_loss']\n",
        "#         print(f\"L: {L}, K: {K}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# section 2.2, Training, Grid Search with residual and standard deviation packed\n",
        "class SearchNN(SimpleNN):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, lr=0.001):\n",
        "        super(SearchNN, self).__init__(input_size, hidden_layers, output_size)\n",
        "        self.loss_function = nn.MSELoss() #temperary: MSE\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def train_single_epoch(self, data_loader):\n",
        "        self.train()\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            self.optimizer.zero_grad()\n",
        "            y_pred = self(x_batch)\n",
        "            loss = self.loss_function(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def validate(self, data_loader):\n",
        "      self.eval()  # Set the model to evaluation mode\n",
        "      val_losses = []\n",
        "      all_residuals = []  # Use a list to collect all residuals\n",
        "\n",
        "      with torch.no_grad():  # No gradients needed for validation\n",
        "          for x_val, y_val in data_loader:\n",
        "              y_val_pred = self(x_val)\n",
        "              val_loss = self.loss_function(y_val_pred, y_val)\n",
        "              val_losses.append(val_loss.item())\n",
        "\n",
        "              # Calculate residuals and append them to the list\n",
        "              residuals = y_val - y_val_pred\n",
        "              all_residuals.append(residuals)\n",
        "\n",
        "      # Concatenate all residuals tensors to form a single tensor\n",
        "      all_residuals_tensor = torch.cat(all_residuals, dim=0)\n",
        "\n",
        "      # Calculate the standard deviation of residuals\n",
        "      # This standard deviation is a measure of how spread out these errors are.\n",
        "      standard_deviation = torch.sqrt(torch.mean(all_residuals_tensor ** 2))\n",
        "\n",
        "      # The mean validation loss\n",
        "      mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "\n",
        "      # Save the residuals and standard deviation as attributes for later use\n",
        "      self.residuals = all_residuals_tensor\n",
        "      self.standard_deviation = standard_deviation\n",
        "\n",
        "      return mean_val_loss\n",
        "\n",
        "\n",
        "    # def validate(self, data_loader):\n",
        "    #     self.eval()\n",
        "    #     val_losses = []\n",
        "    #     y_val_pred_np = np.array([])\n",
        "    #     self.residuals = torch.Tensor()\n",
        "    #     with torch.no_grad():\n",
        "    #         for x_val, y_val in data_loader:\n",
        "    #             y_val_pred = self(x_val)\n",
        "    #             val_loss = self.loss_function(y_val_pred, y_val)\n",
        "    #             val_losses.append(val_loss.item())\n",
        "\n",
        "    #             y_val_pred_np = y_val_pred.detach().cpu().numpy()\n",
        "    #             y_val_np = y_val.cpu().numpy()\n",
        "\n",
        "    #             self.residuals = torch.cat((self.residuals,  torch.tensor(y_val_np - y_val_pred_np)), 0)\n",
        "    #     self.standard_deviation = torch.sqrt(torch.sum(torch.pow(self.residuals, 2)) / (self.residuals.numel() - 1))\n",
        "    #     return sum(val_losses) / len(val_losses)\n",
        "\n",
        "class Repro():\n",
        "  def train_and_evaluate_nn(self, grid_search_param, train_loader, val_loader, num_epochs=10):\n",
        "      results = []\n",
        "      L, K = grid_search_param\n",
        "      # Model with L layers and K units\n",
        "      model = SearchNN(input_size, [K] * L, output_size)\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          train_loss = model.train_single_epoch(train_loader)\n",
        "          #accuracy =\n",
        "      val_loss = model.validate(val_loader)\n",
        "      residual = model.residuals\n",
        "      standard_dev = model.standard_deviation\n",
        "      size = model.residuals.numel()\n",
        "\n",
        "      # Define the nuclear mapping function given (L,K) on Da:\n",
        "      # l(L,K) = -n * log(sqrt(2*pi)*std_dev) - sum(residual**2) / 2*std_dev**2\n",
        "      likelihood = -size * np.log(np.sqrt(2*np.pi)*standard_dev) - np.sum(torch.pow(residual,2).tolist())/ 2*std_dev**2\n",
        "\n",
        "\n",
        "      # results\n",
        "      result = {\n",
        "          'L': L,\n",
        "          'K': K,\n",
        "          'train_loss': train_loss,\n",
        "          'val_loss': val_loss,\n",
        "          'residual': residual,\n",
        "          'standard_dev': standard_dev,\n",
        "          'log_likelihood': likelihood\n",
        "\n",
        "      }\n",
        "      return result\n",
        "\n",
        "  # def repro_data(self,data,repro_quantity, est_std_dev, seed = 39):\n",
        "  #   #seed for torch with/o gpu senario\n",
        "  #   torch.manual_seed(seed)\n",
        "  #   torch.cuda.manual_seed_all(seed)\n",
        "  #   # is it the same for calling a random number generator 100 time as calling random generator once for generating 100 random number?\n",
        "  #   # Yes.\n",
        "  #   self.noise_std = est_std_dev\n",
        "  #   self.original_data = data\n",
        "  #   self.noise = {}\n",
        "  #   self.newdata_cache = {}\n",
        "\n",
        "  #   for repro_index in range(repro_quantity):\n",
        "  #     self.noise[repro_index] = self.noise_std * torch.randn_like(Da.tensors[1])\n",
        "  #     self.newdata_cashe[repro_index] = self.noise[repro_index] + data\n",
        "  #   return self.newdata_cache\n",
        "\n",
        "  def repro_data(self, data, repro_quantity, est_std_dev, seed=39):\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    self.noise_std = est_std_dev\n",
        "    self.original_data = data\n",
        "    self.noise = {}\n",
        "    self.newdata_cache = {}\n",
        "\n",
        "    # Extracting tensors from the original TensorDataset\n",
        "    original_data_tensors = data.tensors\n",
        "\n",
        "    for repro_index in range(repro_quantity):\n",
        "        # Generating noise for each tensor in the dataset\n",
        "        noise_tensors = tuple(self.noise_std * torch.randn_like(tensor) for tensor in original_data_tensors)\n",
        "        self.noise[repro_index] = noise_tensors\n",
        "\n",
        "        # Adding noise to the original data tensors\n",
        "        synthetic_data_tensors = tuple(tensor + noise for tensor, noise in zip(original_data_tensors, noise_tensors))\n",
        "        self.newdata_cache[repro_index] = TensorDataset(*synthetic_data_tensors)\n",
        "\n",
        "    return self.newdata_cache\n",
        "\n",
        "\n",
        "  def monte_carlo(self,data,n_repro,est_std_dev,l_low=1,l_up=4,k_low=3,k_up=6,epoch=5):\n",
        "    self.grid_search_params = [(l,k) for l in range(l_low, l_up) for k in range(k_low,k_up)]\n",
        "    self.n_epoch = epoch\n",
        "    self.n_repro = n_repro\n",
        "    self.repro_results = {}\n",
        "    for param in self.grid_search_params:\n",
        "      data_cache = self.repro_data(data,n_repro,est_std_dev)\n",
        "      for repro_index, synthetic_data in data_cache.items():\n",
        "        repro_train_loader = DataLoader(synthetic_data, batch_size=100, shuffle=True)\n",
        "        self.repro_results[(param,repro_index)] = (repro_index, self.train_and_evaluate_nn(param, repro_train_loader, val_loader, self.n_epoch))\n",
        "\n",
        "    for (l, k), repro_result_tuple in self.repro_results.items():\n",
        "        repro_index, result = repro_result_tuple\n",
        "        train_loss = result['train_loss']\n",
        "        val_loss = result['val_loss']\n",
        "        residual = result['residual']\n",
        "        standard_dev = result['standard_dev']\n",
        "        likelihood = result['log_likelihood']\n",
        "\n",
        "        print(f\"Architecture:{l}, Repro Index:{repro_index}, Train Loss: {train_loss}, Val Loss: {val_loss}, Std Dev: {standard_dev}, Log Likelihood: {likelihood}\")\n",
        "\n",
        "    #   #check single element tensor and convert to float\n",
        "    # if isinstance(residual, torch.Tensor) and residual.numel() == 1:\n",
        "    #     residual = residual.item()\n",
        "    # if isinstance(standard_dev, torch.Tensor) and standard_dev.numel() == 1:\n",
        "    #     standard_dev = standard_dev.item()\n",
        "      # print(f\"L:{l}, K:{k}, repro index:{repro_index}, Std:{standard_dev:.4f}, log_likelihood:{likelihood:.4f}\")\n",
        "    # for futher ulization\n",
        "    return self.repro_results\n",
        "\n",
        "  def borel_confidence_interval(self, data, alpha = 0.95): # is borel set a confidence interval or a set?\n",
        "    self.data = np.array(data)\n",
        "    self.mean = np.mean(data)\n",
        "    self.n = len(data)\n",
        "    self.std_dev = np.std(data, ddof=1)\n",
        "    self.z_score = stats.norm.ppf(1 - (1-alpha)/2) # for over 30, distribution is approx Normal\n",
        "    self.margin_of_error = self.z_score * (std_dev / np.sqrt(self.n))\n",
        "    self.confidence_interval = (self.mean - self.margin_of_error, self.mean + self.margin_of_error)\n",
        "    return self.confidence_interval\n",
        "\n",
        "# val_loader require initialization\n"
      ],
      "metadata": {
        "id": "lq81A-9Ar9oH"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myrepro = Repro()\n",
        "est_std_dev = Db[:][1].std().item()\n",
        "n_repro = 5 #should be more than 30, ideally set to 1000\n",
        "print(est_std_dev)\n",
        "repro_result = myrepro.monte_carlo(Db,n_repro,est_std_dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96vnU7r84Fz4",
        "outputId": "2f89c1f2-100f-4d0f-a4a4-d1c56c5a4ee0"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.305342674255371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([100, 3])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:(1, 3), Repro Index:0, Train Loss: 4.0392680168151855, Val Loss: 1.7020599842071533, Std Dev: 1.304630160331726, Log Likelihood: -39346.484375\n",
            "Architecture:(1, 3), Repro Index:1, Train Loss: 3.473264455795288, Val Loss: 1.701017851829529, Std Dev: 1.304230809211731, Log Likelihood: -39328.6796875\n",
            "Architecture:(1, 3), Repro Index:2, Train Loss: 3.5579569339752197, Val Loss: 1.7073483896255492, Std Dev: 1.3066554069519043, Log Likelihood: -39436.78125\n",
            "Architecture:(1, 3), Repro Index:3, Train Loss: 3.355862855911255, Val Loss: 1.7019293689727784, Std Dev: 1.3045802116394043, Log Likelihood: -39344.25390625\n",
            "Architecture:(1, 3), Repro Index:4, Train Loss: 3.5742745399475098, Val Loss: 1.708633198738098, Std Dev: 1.3071469068527222, Log Likelihood: -39458.703125\n",
            "Architecture:(1, 4), Repro Index:0, Train Loss: 3.8577840328216553, Val Loss: 1.7084450769424437, Std Dev: 1.307075023651123, Log Likelihood: -39455.49609375\n",
            "Architecture:(1, 4), Repro Index:1, Train Loss: 3.076303482055664, Val Loss: 1.7044801378250123, Std Dev: 1.305557370185852, Log Likelihood: -39387.8125\n",
            "Architecture:(1, 4), Repro Index:2, Train Loss: 3.233877420425415, Val Loss: 1.7063757014274596, Std Dev: 1.3062831163406372, Log Likelihood: -39420.1796875\n",
            "Architecture:(1, 4), Repro Index:3, Train Loss: 3.2152602672576904, Val Loss: 1.7253126454353334, Std Dev: 1.3135114908218384, Log Likelihood: -39742.9765625\n",
            "Architecture:(1, 4), Repro Index:4, Train Loss: 3.1402573585510254, Val Loss: 1.763032636642456, Std Dev: 1.3277924060821533, Log Likelihood: -40383.28125\n",
            "Architecture:(1, 5), Repro Index:0, Train Loss: 3.5481574535369873, Val Loss: 1.7074392032623291, Std Dev: 1.3066902160644531, Log Likelihood: -39438.33203125\n",
            "Architecture:(1, 5), Repro Index:1, Train Loss: 3.2836849689483643, Val Loss: 1.701493284702301, Std Dev: 1.304413080215454, Log Likelihood: -39336.8046875\n",
            "Architecture:(1, 5), Repro Index:2, Train Loss: 2.788616180419922, Val Loss: 1.7512557768821717, Std Dev: 1.323350191116333, Log Likelihood: -40183.7421875\n",
            "Architecture:(1, 5), Repro Index:3, Train Loss: 3.310469150543213, Val Loss: 1.700588619709015, Std Dev: 1.3040661811828613, Log Likelihood: -39321.34375\n",
            "Architecture:(1, 5), Repro Index:4, Train Loss: 3.8004956245422363, Val Loss: 1.7159472942352294, Std Dev: 1.3099416494369507, Log Likelihood: -39583.44921875\n",
            "Architecture:(2, 3), Repro Index:0, Train Loss: 3.981973886489868, Val Loss: 1.7268059253692627, Std Dev: 1.3140798807144165, Log Likelihood: -39768.39453125\n",
            "Architecture:(2, 3), Repro Index:1, Train Loss: 3.4551830291748047, Val Loss: 1.8592298746109008, Std Dev: 1.363535761833191, Log Likelihood: -42001.03125\n",
            "Architecture:(2, 3), Repro Index:2, Train Loss: 3.477066993713379, Val Loss: 1.7334732604026795, Std Dev: 1.3166143894195557, Log Likelihood: -39881.8046875\n",
            "Architecture:(2, 3), Repro Index:3, Train Loss: 3.322252035140991, Val Loss: 1.6991635370254516, Std Dev: 1.3035197257995605, Log Likelihood: -39297.0\n",
            "Architecture:(2, 3), Repro Index:4, Train Loss: 3.9952969551086426, Val Loss: 1.7231243705749513, Std Dev: 1.312678337097168, Log Likelihood: -39705.72265625\n",
            "Architecture:(2, 4), Repro Index:0, Train Loss: 3.5299699306488037, Val Loss: 1.7147709107398987, Std Dev: 1.309492588043213, Log Likelihood: -39563.39453125\n",
            "Architecture:(2, 4), Repro Index:1, Train Loss: 3.7147912979125977, Val Loss: 1.7004592776298524, Std Dev: 1.3040165901184082, Log Likelihood: -39319.13671875\n",
            "Architecture:(2, 4), Repro Index:2, Train Loss: 3.588754177093506, Val Loss: 1.7018833947181702, Std Dev: 1.3045625686645508, Log Likelihood: -39343.46875\n",
            "Architecture:(2, 4), Repro Index:3, Train Loss: 3.1821227073669434, Val Loss: 1.741594033241272, Std Dev: 1.3196946382522583, Log Likelihood: -40019.7890625\n",
            "Architecture:(2, 4), Repro Index:4, Train Loss: 3.365731716156006, Val Loss: 1.6995142364501954, Std Dev: 1.3036541938781738, Log Likelihood: -39302.9921875\n",
            "Architecture:(2, 5), Repro Index:0, Train Loss: 3.7078144550323486, Val Loss: 1.702156858444214, Std Dev: 1.304667353630066, Log Likelihood: -39348.140625\n",
            "Architecture:(2, 5), Repro Index:1, Train Loss: 3.802171230316162, Val Loss: 1.700003182888031, Std Dev: 1.3038417100906372, Log Likelihood: -39311.34375\n",
            "Architecture:(2, 5), Repro Index:2, Train Loss: 3.284547805786133, Val Loss: 1.7017176055908203, Std Dev: 1.3044990301132202, Log Likelihood: -39340.6328125\n",
            "Architecture:(2, 5), Repro Index:3, Train Loss: 3.6182308197021484, Val Loss: 1.6997398447990417, Std Dev: 1.3037407398223877, Log Likelihood: -39306.84375\n",
            "Architecture:(2, 5), Repro Index:4, Train Loss: 3.54670786857605, Val Loss: 1.7100485491752624, Std Dev: 1.3076882362365723, Log Likelihood: -39482.85546875\n",
            "Architecture:(3, 3), Repro Index:0, Train Loss: 3.7727649211883545, Val Loss: 1.7003040838241577, Std Dev: 1.3039571046829224, Log Likelihood: -39316.484375\n",
            "Architecture:(3, 3), Repro Index:1, Train Loss: 3.470175266265869, Val Loss: 1.7135836052894593, Std Dev: 1.3090392351150513, Log Likelihood: -39543.1484375\n",
            "Architecture:(3, 3), Repro Index:2, Train Loss: 3.4754931926727295, Val Loss: 1.703512694835663, Std Dev: 1.3051868677139282, Log Likelihood: -39371.296875\n",
            "Architecture:(3, 3), Repro Index:3, Train Loss: 2.9400694370269775, Val Loss: 1.7807629823684692, Std Dev: 1.3344522714614868, Log Likelihood: -40683.0625\n",
            "Architecture:(3, 3), Repro Index:4, Train Loss: 3.281930685043335, Val Loss: 1.6989478588104248, Std Dev: 1.3034369945526123, Log Likelihood: -39293.3125\n",
            "Architecture:(3, 4), Repro Index:0, Train Loss: 3.676199197769165, Val Loss: 1.7002837324142457, Std Dev: 1.303949236869812, Log Likelihood: -39316.13671875\n",
            "Architecture:(3, 4), Repro Index:1, Train Loss: 2.9989447593688965, Val Loss: 1.6988219165802, Std Dev: 1.3033885955810547, Log Likelihood: -39291.16015625\n",
            "Architecture:(3, 4), Repro Index:2, Train Loss: 3.1168079376220703, Val Loss: 1.6992145085334778, Std Dev: 1.3035392761230469, Log Likelihood: -39297.8671875\n",
            "Architecture:(3, 4), Repro Index:3, Train Loss: 3.563210964202881, Val Loss: 1.699943127632141, Std Dev: 1.303818702697754, Log Likelihood: -39310.3203125\n",
            "Architecture:(3, 4), Repro Index:4, Train Loss: 3.4973604679107666, Val Loss: 1.699269061088562, Std Dev: 1.3035601377487183, Log Likelihood: -39298.80078125\n",
            "Architecture:(3, 5), Repro Index:0, Train Loss: 3.8600032329559326, Val Loss: 1.699678931236267, Std Dev: 1.3037173748016357, Log Likelihood: -39305.8046875\n",
            "Architecture:(3, 5), Repro Index:1, Train Loss: 3.7541751861572266, Val Loss: 1.7024468874931336, Std Dev: 1.3047784566879272, Log Likelihood: -39353.08984375\n",
            "Architecture:(3, 5), Repro Index:2, Train Loss: 3.0395121574401855, Val Loss: 1.701318655014038, Std Dev: 1.3043460845947266, Log Likelihood: -39333.8203125\n",
            "Architecture:(3, 5), Repro Index:3, Train Loss: 3.4410176277160645, Val Loss: 1.6999899792671203, Std Dev: 1.3038365840911865, Log Likelihood: -39311.1171875\n",
            "Architecture:(3, 5), Repro Index:4, Train Loss: 3.402420997619629, Val Loss: 1.7011215901374817, Std Dev: 1.3042705059051514, Log Likelihood: -39330.453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def borel_confidence_interval(self, data, alpha = 0.95): # is borel set a confidence interval or a set?\n",
        "#     self.data = np.array(data)\n",
        "#     self.mean = np.mean(data)\n",
        "#     self.n = len(data)\n",
        "#     self.std_dev = np.std(data, ddof=1)\n",
        "#     self.z_score = stats.norm.ppf(1 - (1-alpha)/2) # for over 30, distribution is approx Normal\n",
        "#     self.margin_of_error = self.z_score * (std_dev / np.sqrt(self.n))\n",
        "#     self.confidence_interval = (self.mean - self.margin_of_error, mean + self.margin_of_error)\n",
        "#     return self.confidence_interval"
      ],
      "metadata": {
        "id": "xnEWGmMLNOqX"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empirical Distribution of Likelihood\n",
        "likelihood_distribution = {}\n",
        "lt = []\n",
        "for (l, k), repro_result_tuple in repro_result.items():\n",
        "    # temp = []\n",
        "    repro_index, result = repro_result_tuple\n",
        "    lt.append(result['log_likelihood'].numpy())\n",
        "    if repro_index == n_repro-1:\n",
        "        likelihood_distribution[l] = tuple(lt)\n",
        "        lt.clear()"
      ],
      "metadata": {
        "id": "0M-F3ydqmQOV"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "borel_intervals = {}\n",
        "for archtect, likelihoods in likelihood_distribution.items():\n",
        "    data = list(likelihoods)\n",
        "    interval = myrepro.borel_confidence_interval(data)\n",
        "    borel_intervals[archtect] = interval\n",
        "\n",
        "print(borel_intervals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmWrhvGi4BEo",
        "outputId": "b63e2ef7-f981-4b82-9ee7-5315b1cf3f5a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 3): (-39384.11994805275, -39381.84098944725), (1, 4): (-39679.08869805275, -39676.80973944725), (1, 5): (-39573.87385430275, -39571.59489569725), (2, 3): (-40131.92854180275, -40129.64958319725), (2, 4): (-39510.89729180275, -39508.61833319725), (2, 5): (-39359.10432305275, -39356.82536444725), (3, 3): (-39642.60041680275, -39640.32145819725), (3, 4): (-39303.99494805275, -39301.71598944725), (3, 5): (-39327.99494805275, -39325.71598944725)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wxVCcKdCkXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#\n",
        "\n",
        "\n",
        "#Stand alone version\n",
        "# def train_and_evaluate_nn(grid_search_param, train_loader, val_loader, num_epochs=10):\n",
        "#     results = []\n",
        "#     L, K = grid_search_param\n",
        "#     # Model with L layers and K units\n",
        "#     model = SearchNN(input_size, [K] * L, output_size)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss = model.train_single_epoch(train_loader)\n",
        "#         #accuracy =\n",
        "#     val_loss = model.validate(val_loader)\n",
        "#     residual = model.residuals\n",
        "#     standard_dev = model.standard_deviation\n",
        "#     size = model.residuals.numel()\n",
        "\n",
        "#     # Define the nuclear mapping function given (L,K) on Da:\n",
        "#     # l(L,K) = -n * log(sqrt(2*pi)*std_dev) - sum(residual**2) / 2*std_dev**2\n",
        "#     likelihood = -size * np.log(np.sqrt(2*np.pi)*standard_dev) - np.sum(torch.pow(residual,2).tolist())/ 2*std_dev**2\n",
        "\n",
        "\n",
        "#     # results\n",
        "#     result = {\n",
        "#         'L': L,\n",
        "#         'K': K,\n",
        "#         'train_loss': train_loss,\n",
        "#         'val_loss': val_loss,\n",
        "#         'residual': residual,\n",
        "#         'standard_dev': standard_dev,\n",
        "#         'log_likelihood': likelihood\n",
        "\n",
        "#     }\n",
        "#     return result\n",
        "\n",
        "\n",
        "# # Grid search\n",
        "# grid_search_params = [(l, k) for l in range(1, 4) for k in range(3, 6)] # Small 3x3 grid search\n",
        "# num_epochs = 5  # Temporary\n",
        "# evaluation_results = {}\n",
        "# for param in grid_search_params:\n",
        "#     evaluation_results[param] = train_and_evaluate_nn(param, train_loader, val_loader, num_epochs)\n",
        "\n",
        "# for (L, K), result in evaluation_results.items():\n",
        "#     train_loss = result['train_loss']\n",
        "#     val_loss = result['val_loss']\n",
        "#     residual = result['residual']\n",
        "#     standard_dev = result['standard_dev']\n",
        "#     likelihood = result['log_likelihood']\n",
        "\n",
        "#     #check single element tensor and convert to float\n",
        "#     if isinstance(residual, torch.Tensor) and residual.numel() == 1:\n",
        "#         residual = residual.item()\n",
        "#     if isinstance(standard_dev, torch.Tensor) and standard_dev.numel() == 1:\n",
        "#         standard_dev = standard_dev.item()\n",
        "\n",
        "#     print(f\"L: {L}, K: {K}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Residuals: {residual}, Std: {standard_dev:.4f}, log_likelihood: {likelihood: .4f}\")"
      ],
      "metadata": {
        "id": "hxXwa12v2rAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tmF-SdWIWjB"
      },
      "outputs": [],
      "source": [
        "# # # Repro with nosie (Monte-calo)\n",
        "# def noiseGenerate(self, size, seed, mean = 0, cov=[[],[]], distribution = 'normal'): # cov need be adjusted to the layer dimensions for each layers\n",
        "#   self.seed = seed\n",
        "#   self.mean = mean\n",
        "#   self.size = size\n",
        "#   self.cov = cov\n",
        "\n",
        "# # # Use Quasi Monte-Calo\n",
        "#   dist = stats.qmc.MultivariateNormalQMC(self.mean, seed=self.seed, cov=self.cov)\n",
        "#   noise = dist.random(size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n10Khbgl2Ya-"
      },
      "outputs": [],
      "source": []
    }
  ]
}